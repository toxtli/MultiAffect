{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiAffect: Reproducible Research Framework for Multimodal Video Categorization and Regression Tasks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be238XGhoFrm",
        "colab_type": "text"
      },
      "source": [
        "# MultiAffect: Reproducible Research Framework for Multimodal Video Categorization and Regression Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0HbfhYBDiE7",
        "colab_type": "text"
      },
      "source": [
        "Author1, Author2, and Author3\n",
        "\n",
        "Institution Name\n",
        "\n",
        "{author1,author2,author3}@institution.email\n",
        "\n",
        "[http://www.carlostoxtli.com](http://www.carlostoxtli.com)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJUZsAZRDXeB",
        "colab_type": "text"
      },
      "source": [
        "# Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0mrnrqWEMrq",
        "colab_type": "text"
      },
      "source": [
        "Although access to information and to distributed computing resources are becoming more and more open, there is still a widespread perception of a reproducibility crisis for computational studies. Researchers over the years have investigated the factors that affect reproducibility in data related studies. Some common findings point that non-reproducible studies lack information on access to the dataset in the form and or- der as used in the original study, the software environment used, ran- domization control, and the implementation of proposed techniques. In addition to that, some studies require a large amount of computational resources that not everybody can afford. Our work explores how to over- come some of the main challenges in reproducible research related to multimodal video prediction. We present MultiAffect, an inclusive repro- ducible research framework that standardizes the platform setup, feature extraction techniques, training and evaluation methods, and the research document formatting for multimodal video prediction tasks in an online environment. The framework was designed to use a simple vanilla version of popular algorithms, but it is able to plug state-of-the-art algorithms into the workflow with ease. We tested the framework for two different video analysis approaches that are affect recognition and video action recognition. MultiAffect was able to perform both tasks by only chang- ing the configuration. The results produced by MultiAffect were compet- itive in regard to published studies. MultiAffect was deployed in Google Colaborarity where we validate inclusiveness by being able to reproduce experiments with no client requirements (on- line), zero-configuration, and free-of-charge. We aim that inclusive repro- ducible research frameworks for complex and highly demanding tasks, can reduce the barrier of entry and boost the progress in this area."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcOhF0uDQUn9",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37vMfzpYQk49",
        "colab_type": "text"
      },
      "source": [
        "The affective computing term was coined by Rosalind Picard in 1995 [33] to study the role that affect plays in intelligent behavior. It explores how to auto- matically recognize human affect, and how to generate corresponding responses by the computer. Performing automatic affect recognition relies on techniques commonly expected of AI, such as auditory and visual perception [14]. Machine learning approaches enable computers to learn directly from examples, instead  of providing explicit models. Humans intuitively recognize affect from visual  and auditory cues, but computers need to have a digital representation that is usually high dimensional. The curse of dimensionality is a phenomenon where the higher the number of dimensions of the data, the less effective conventional computational and statistical methods become [13]. A common solution to this problem is to project the high-dimensional data into a lower-dimensional space through approaches such as feature selection.\n",
        "\n",
        "Machine learning algorithms with so-called shallow architectures, such as ker- nel methods and single-layer neural networks often rely on handcrafted features based upon heuristics of the target problem. These approaches may not always the most efficient way to approach some challenging learning problems such as affect recognition [14]. Since 2010 researchers started to explore the application of deep architectures for affect recognition. Machine learning techniques applied to affect recognition are widely applied, it is estimated that between 2010 and 2017, around 950 studies were published in this area [35]. In our work, we im- plemented deep learning approaches for our affect recognition models.\n",
        "\n",
        "Instantaneous emotion categorization is performed in short-term videos that last usually a couple of seconds [41, 42]. These short clips conditions use short utterances instead of natural utterances [25, 23]. An utterance is the smallest unit of speech. It is a continuous piece of speech beginning and ending with a clear pause. Humans have the capability to adapt their internal emotion representation to a newly perceived emotional expression on the fly and use it to obtain a greater understanding of the emotional behavior of another person. This mechanism is described as a developmental learning process. Algorithms that can understand emotions as humans should be able to process and understand emotions from long-term natural utterances.\n",
        "\n",
        "The notion of categorical affective states originated in Charles Darwin’s re- search on the evolution of affect, and it still remains discussed in the literature [29]. The six universal emotions categorization scheme was originally proposed by Paul Ekman and was based on facial expressions [21]. It includes Disgust, Fear, Happiness, Surprise, Sadness, and Anger. Humans usually express them- selves differently, sometimes even combining one or more characteristics of the so-called universal emotions, this can be described by dimensional emotion rep- resentations. Dimensional models aim to avoid the restrictiveness of discrete states and allow a more flexible definition of affective states as points in a multi- dimensional space. The most commonly used example is Russells circumplex model [36], which consists of the two dimensions valence and arousal. The OMG Emotion dataset includes categorical and dimensional annotations. Each utter- ance contains arousal/valence value in dimensional space, as well as seven dis- crete emotion labels. Arousal is a continuous score ranging from 0 (calm) to 1 (excited), while valence is a continuous score ranging from -1 (negative) to +1 (positive). Two following metrics are used to evaluate the arousal/valence esti- mation over this dataset: MSE (mean squared error) and CCC (the concordance correlation coefficients). Our work uses categorical and dimensional annotations to build the models.\n",
        "\n",
        "Humans rely on multiple modalities when expressing and sensing affective states in social interactions. It seems natural that computers could benefit from the same variety of sensors [30]. In fact, there has been an increased interest to design such multimodal systems [19], and it is generally accepted that audiovi- sual sensor fusion can increase model robustness and accuracy. The challenge in joint multimodal feature learning is how and at what stage to fuse data from multiple modalities. Fusion can be achieved at early model stages close to the raw sensor data, or at a later stage by combining independent models. In early or feature-level fusion, features are extracted independently and then concatenated for further learning of a joint feature representation; this allows the model to capture correlations between the modalities. Late or decision-level fusion aggre- gates the results of independent recognition models. Our work uses ten modal- ities extracted from images, audio, and text. Some features are spatiotemporal frame-level sequences while others are aggregated utterance-level features. The used features are: face deep features, face handcrafted features, body skeleton joint angles, body skeleton deep features, word MPQA opinion lexicon, word Bing Liu lexicon, audio features per utterance, audio features per fragments, in- stantaneous emotions per utterance, and instantaneous emotions per fragments. For each feature we performed unimodal and multimodal tests. We implemented LSTM (Long short-term memory) for spatiotemporal features and MLP (Multi- Layer Perceptrons) for aggregated features. For multimodal tests, we performed early and late fusions.\n",
        "\n",
        "According to [9] there is a general perception of a reproducibility crisis. Their studies show that around 70% of researchers have tried and failed to reproduce another scientist’s experiment. A manifesto for reproducible science states that improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery [28]. There have been important advances in promoting platforms to reproduce com- putational workflows from digital notebooks such as Jupyter [26]. Reproducing multimodal affect recognition is a particularly complex task since it requires the configuration of multiple tools and a lot of resources in terms of data stor- age and processing. With that aim in mind, we  structured a dynamic version  of this chapter that can be found at [1]. The dynamic version is designed as a framework that defines guidelines to feature extraction and training processes. It takes advantage of powerful online platforms such as Google Colaboratory to set up the virtual machine, load the data, extract the features, and train the model, all from the same notebook.\n",
        "\n",
        "We conclude our work showing design principles for reproducible research frameworks in the video analysis space, and how this work can be expanded to other fields. We did not perform all the possible feature combinations to open the possibility to others to build on top of this work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXvDYezKQxy9",
        "colab_type": "text"
      },
      "source": [
        "# MultiAffect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61TLSYrTQ0KQ",
        "colab_type": "text"
      },
      "source": [
        "MultiAffect is a reproducible research framework for computational workflows based on multimodal video categorization and regression tasks. The main goal of MultiAffect is to reproduce research experiments in a fixed setting. In order to achieve that goal, the MultiAffect framework considers the following compo- nents: (1) Research paper template: Defines the minimum set of sections and mandatory citations; (2) Platform Setup: Ensures that the machine is properly configured; (3) Feature Extractor: Monitors the feature extraction and manage the extracted features; (4) Model Trainer: Defines, tunes, and trains the model;\n",
        "\n",
        "(5) Evaluator: Calculates and reports the performance metrics. Figure 1 shows an interface screenshot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9XVg8IDV7T5",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper01.png)\n",
        "\n",
        "Fig. 1. MultiAffect interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqncYNGoWqDT",
        "colab_type": "text"
      },
      "source": [
        "## Research paper template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZC0BtcnW74X",
        "colab_type": "text"
      },
      "source": [
        "Reproducible research papers have two main components, the explanatory docu- ment and an infrastructure attached to the document. Interactive notebooks are ideal to achieve this goal, and Jupyter is one of the most used platforms for this purpose. Jupyter is an open-source online platform that enables users to create and share documents that contain live code, equations, visualizations, and nar- rative text. The notebooks created in Jupyter can be styled by using Markdown syntax. Markdown is a lightweight markup language with plain text formatting syntax. The goal of Markdown is to enable people to write using an easy-to-read and easy-to-write plain text format. However, research paper formats are usually written using LaTeX syntax. LaTeX is a typesetting system designed for the pro- duction of technical and scientific documentation. LaTeX needs to be compiled to build a formatted human-readable version of the document. There are some formatting tasks that are needed to properly adapt the interactive notebook to the research paper template. These tasks are time-consuming and do not add value to the research work. MultiAffect rids off all of these formatting tasks to help researchers to focus on the experimentation and divulgation part of their work.\n",
        "\n",
        "There are some challenges related to the research paper template replication in interactive notebooks, such as the wide variety of formats and the number  of columns. Although two-column research paper formats are commonly used for some artificial intelligence proceedings, there is currently challenging for an interactive notebook to replicate that format. Markdown syntax is mainly fo- cused on one-column format. The standard version of Jupyter notebook does not include graphical tools to split texts into two columns. Even though some HTML tricks can be done to structure in an equivalent format, it loses their responsiveness across devices. For the initial version of MultiAffect framework, we proposed a basic version and a set of rules that can help to expand its ca- pabilities. The proposed solution replicates a one-column ACM (Association for Computing Machinery) manuscript format. The reason behind this decision is the number of HCI (Human-Computer Interaction) proceedings available and its impact on affective computing.\n",
        "\n",
        "The MultiAffect template contains six main sections that are: (1) Abstract: Overall description of a video action recognition paper; (2) Introduction: Ex- tended description of a video action recognition approach that implements fused multimodal features; (3) Methodology: Explains the feature extraction and train- ing models applied; (4) Results: Generic explanatory text, a benchmark table, and performance plots; (5) Conclusions: Give a general conclusion of how fused multimodal models are useful for action recognition; and (6) References: It al- ready contains the references to the used feature extraction methods, datasets, and algorithms.\n",
        "\n",
        "It is important to mention that although the framework provides a ready to use and publish template, we do not encourage users to submit slightly edited versions to proceedings. Incremental research best practices should be considered before submitting a derived version from this framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHSESVfYXFau",
        "colab_type": "text"
      },
      "source": [
        "## Platform Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou0Hs970XHne",
        "colab_type": "text"
      },
      "source": [
        "Preparing a machine to replicate artificial intelligent research work is usually challenging. Since most of the machine learning approaches available today are based on learning from multiple samples, multimedia datasets often requires high storage requirements. Feature extraction tasks help algorithms to reduce dimensionality and to focus on the most significant or discriminative parameters. Extracting features from multimedia samples is a highly demanding task in terms of computing power. Some of the tools that are required to perform the data extraction need to be compiled for the host operating system. Scientific tools are commonly built from multiple libraries and sometimes depend on specific versions of certain libraries for certain operating systems, this makes them prone to throw compilation errors. Sometimes the code is not given and there is an extra effort to code the instructions described in the publication. Even if the code is available, sometimes the code is not ready to reproduce and important efforts should be performed to make it work, when works.\n",
        "\n",
        "The software challenges can be mitigated by using virtual machines or con- tainers. Virtual machines and containers give a base operating system that can contain the proper configuration built-in. These approaches can run in the top of the host operating system or in online infrastructure. The hardware challenges can be overcome by investing in powerful enough architecture in-site or by us- ing online on-demand infrastructure. Conventional research paper replication depends on multiple factors as we have explored.\n",
        "\n",
        "The MultiAffect framework uses Google Colaboratory to publish the interac- tive notebook and to perform the computation in the attached virtual machine. Google Colaboratory is a free research tool that enables users with a Google account to host and run code over Google’s infrastructure. Google Colaboratory offers to users the ability to execute their code segments in CPUs, GPUs, and TPUs (an AI accelerator application-specific  integrated  circuit).  By  the  time this work is published, Google Colaboratory offers a virtual machine with a    Tesla K80 GPU, 12 GB of RAM memory, and 350 GB of storage. This platform provides enough resources to perform the computation required for multimodal analysis of video, and the storage required for storing the 8.4 Gb required for     the OMG-Emotion dataset used for showcasing this work.\n",
        "\n",
        "Our framework configures the Google Colaboratory’s built-in virtual machine with the packages that are required to perform the feature extraction. This plat- form includes a Debian based operating setting, so the provided instructions are platform specific. Local replication of our framework requires an Ubuntu 18.04 operating system in order to successfully install all the libraries. Our platform is agnostic to the Python version, all the code executed in the notebook is written in Python and it can be executed in the versions 2 or 3 of the interpreter. Our framework is able to set up and run the experiment from the online platform, enabling users to deploy and execute the code in a free of charge environment and without special requirements in the client side.\n",
        "\n",
        "Some of the pre-requisites are libraries that are already available in package managers. The operating systems packages were installed by using the apt-get package manager and the Python libraries by using the pip tool. The required packages and libraries were installed from a single line command containing all the package or library names separated by a space. The tools that were downloaded and processed in the host (compiled or adapted) were: OpenSMILE, an audio toolkit that needs to be compiled; OpenFace a toolkit for FER (Face Ex- pression Recognition) applications that needs to be compiled; and VGG-Face, a tool to extract deep features from face images, it needs to be edited to sup- port the latest Keras (a deep learning framework) version. Additional work was needed to prepare the Google Collaboratory environment to be ready to compile the tools and run the code. It was needed to upgrade the cmake tool version,    a tool that commands the compilation and building processes. The OpenCV   (a popular computer vision library) was needed to be upgraded to the latest version.\n",
        "\n",
        "Other libraries that were used to extract certain features such as poses or in- stantaneous emotions were loaded as pre-trained models. We implemented Caffe and Keras pre-trained models. The Caffe models were implemented by using OpenCV DNN as an interface that loads a caffemodel and a prototxt files. The Keras models were loaded as checkpoints (hdf5 format) instead of only weights (h5 format). The main difference between both approaches is that checkpoint files include the network architecture and the weights, and the weights file re- quires the architecture to be coded. We implemented OpenPose (a pose extractor toolkit) as a Caffe model and five different emotion recognition models in Keras checkpoint format.\n",
        "\n",
        "The setup process was conducted in three steps: (1) Initial setup: The first functional version; (2) Packing components: Uploading components in batches to a cloud storage; and (3) Optimal setup: A version that loads faster.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ooPeJQSXVJb",
        "colab_type": "text"
      },
      "source": [
        "### Initial setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rjCJd4nXY-w",
        "colab_type": "text"
      },
      "source": [
        "In this step the libraries were downloaded and compiled directly from the notebook by running shell commands from the notebook cells. Pre- requisites, missing dependencies, and additional packages were installed in the same notebook. The dataset and the pre-trained models were downloaded from their original sources to the virtual machine. The feature extraction, training,     and evaluation code were directly inserted in the notebook in separated cells.    The first version was tested until it successfully was able to  extract  features, train, and evaluate models from the notebook. A backup of this notebook was documented and versioned as the initial version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PdYKzESXcl5",
        "colab_type": "text"
      },
      "source": [
        "### Packing components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uLxtI2dXesY",
        "colab_type": "text"
      },
      "source": [
        "Each individual compiled library was packaged into a zip file that contains the binary files as well as the configuration files. The pre- trained models that were individually downloaded from their original sources  were packed together into a single file. Sometimes the latency is reduced by downloading a single large file from a high-speed source and increased when downloading multiple large files from different bandwidths. The outcome of this task is a collection of zip files that were uploaded to a Google Drive account.   The files were shared with public access to be able to be downloaded in Google Colaboratory notebooks logged with different accounts. In case that the dataset license allows users to store it in a cloud infrastructure, it should be considered   to reduce the bandwidth load, especially for large video datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Hgpy26XmwK",
        "colab_type": "text"
      },
      "source": [
        "### Optimal setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HTSxDizXo8j",
        "colab_type": "text"
      },
      "source": [
        "After packaging and storing the files from the initial setup to the cloud, we started a branch of the initial setup that loads these files. The optimal setup notebook was a simplified version of the initial notebook, instead of having a long section documenting the setup process, it was replaced with a download pre-requisites section. The files were downloaded by using a Python tool called gdown that is already installed in Google Colaborary. It is impor- tant to mention that the virtual machine attached to the Google Colaboratory notebooks has already an Ubuntu distribution with the most common machine learning tools and libraries already installed. This optimal version is tailored  to Google Colaboratory only, other Ubuntu environments should consider the initial setup since it installs all the required libraries.\n",
        "\n",
        "Per each of the libraries installed, we measured the time that takes to in- stall the pre-requisites plus the compilation time. In average the overall setup of each library was five times slower than downloading and extracting a pre- viously compiled and zipped version of the library. It is important to mention that some datasets requires explicit permission to be downloaded and stored, and must be deleted after using them. This is the case of the OMG-Emotion dataset that should be deleted once it was used. The total setup time for the Google Colaboratory environment was reduced from 43 minutes to 6 minutes after implementing the pre-compiled tools strategy and by downloading the files from the same Google infrastructure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IObejWg_XuV3",
        "colab_type": "text"
      },
      "source": [
        "## Feature extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3a6KBiYXwfu",
        "colab_type": "text"
      },
      "source": [
        "MultiAffect includes the feature extraction module as an independent compo- nent. Multimodal feature extraction is often a highly demanding task. It usually requires some pre-processing in the video before being able to extract features. Some common pre-processing tasks are separating the audio, extracting frames, identifying faces, cropping faces, removing the background, among other tasks. Our feature extraction methodology is based on the commonalities among some solution approaches [39, 32, 24, 43, 17, 16] submitted to the OMG-Emotion Challenge [3].\n",
        "\n",
        "The main aim of our feature extraction process is to maintain as invariant factors the person descriptors (i.e. gender, age, etc), scale, position, background, and language. Our approach considers ten features from five different modalities, that are features from face, body, audio, text, and emotions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBS7kDbGX7J1",
        "colab_type": "text"
      },
      "source": [
        "### Face features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIWua5EoX9HR",
        "colab_type": "text"
      },
      "source": [
        "Visual features consist of OpenFace [11] estimators on the whole frames, and VGG face representation [31] on facial regions. For OpenFace features, we use OpenFace toolkit to extract the estimated 68 facial landmarks in both 2D and 3D world coordinates, eye gaze direction vector in 3D, head pose, rigid head shape, and Facial Action Units intensity [20] indicating the facial muscle movements. The detailed feature descriptions are seen in [4]. Those visual descriptors are regarded as strong indicators of human emotions and sentiments [34, 38]. For the VGG face representation, the facial region in each frame is cropped and aligned using a 3D Constrained Local Model described in [10]. We zero out the background according to the face contour indicated by the facial landmarks. Then, the cropped faces are resized to 224x224x3 and fed into a VGG Face model pre-trained on a large face dataset. We take the 4096-dimensional feature vectors in the fc6 layer and concatenate them with the visual features extracted by OpenFace. The total dimension of the concatenated  features  is  4805. Specifically, 20 frames are uniformly sampled from each video clip and fed into the network for training and testing. In the case of a shorter length of a    video clip, we duplicated the last frame to fill the gap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMEt3gQJX_Dm",
        "colab_type": "text"
      },
      "source": [
        "### Body features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsMbg44JYJG6",
        "colab_type": "text"
      },
      "source": [
        "We used from OpenPose [15] (a pose estimator frame- work) the Body-25 model that extracts 25 joints of the person’s skeleton from  an image. We  computed the joint angles from shoulders, arms, neck, and nose,   as well as a binary flag per each joint to indicate if it was present. In total,         we  extracted and normalized 11 handcrafted features from the OpenPose out-  put. We  only used joints from the nose, ears, eyes, neck, shoulders, and arms,   that are the parts of the body that are usually visible in monologues. From the filtered joints we draw a skeleton with the neck joint fixed at the center, the dimensions normalized. and this skeleton is inserted in a frame of size 224x224 with a black background. This visual representation is passed through a VGG16 net to extract 4096 features. We take the 4096 deep features vector from the fc6 layer and concatenate the 11 features computed from the OpenPose joints. The total dimension of the concatenated features is 4107. 20 frames were uniformly sampled per clip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdhSdz3bYNFr",
        "colab_type": "text"
      },
      "source": [
        "### Emotion features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz2r_98mYSHc",
        "colab_type": "text"
      },
      "source": [
        "We used EmoPy [8] a machine learning toolkit for emotional expression to extract the score of each of the seven basic emotions (anger, fear, disgust, happiness, sadness, and contempt) typically used for Facial Expression Recognition (FER). The same seven features were extracted from other four emotion recognition models [7, 6, 2]. We concatenated all the predic- tions from the models into a 35 features vector. The same 20 faces used for com- puting the face features were used to compute the emotion temporal features. We condensed the temporal features to a general feature vector by performing a normalized sum to each prediction segment resulting in a vector of 35 elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70WOmskXYV8E",
        "colab_type": "text"
      },
      "source": [
        "### Audio features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv3WhPpwYYnw",
        "colab_type": "text"
      },
      "source": [
        "Audio features are extracted using openSMILE toolkit [22], and we use the same feature set as suggested in the INTERSPEECH 2010 paralinguistics challenge [37]. The set contains Mel Frequency Cepstral Coeffi- cients (MFCCs), MFCC, loudness, pitch, jitter, etc. [5]. These features describe the prosodic pattern of different speakers and are consistent signs of their states. For each video clip, we extract 1582 dimensional features from the audio sig- nal. We processed the general features from the whole video clip audio, and the temporal features from the analysis of 20 fragments of the video clip audio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-tzgskAYcQQ",
        "colab_type": "text"
      },
      "source": [
        "### Text features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mJzKm1fYeV_",
        "colab_type": "text"
      },
      "source": [
        "We use two opinion lexicons to analyze the patterns in a language context. The first one is Bing Liu opinion Lexicon [18] with 2006 positive words and 4783 negative words. The second one is MPQA Subjectivity Lexicon [40] with 2718 positive words and 4913 negative words. For each ut- terance, we compute the frequency of positive and negative words according to the two lexicons, as well as the total word number in the whole utterance. For utterances without a transcript, we replicate the transcript of the closest utterance in time. We also extract the word frequencies over the entire video and assign them as features for all utterances in the same video. The total dimension of the word feature is finally 10, including utterance-level and video-level word frequency from two lexicons and the total word counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umqTUlyeYmXG",
        "colab_type": "text"
      },
      "source": [
        "## Model trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPuDZ3wiYoYk",
        "colab_type": "text"
      },
      "source": [
        "The MultiAffect models use several artificial intelligence approaches to recog- nize affect from different modalities. Some of them are RNN (Recurrent Neural Networks), CNN (Convolutional Neural Networks), and MLP (Multi-Layer Per- ceptrons) for deploying DNN (Deep Neural Networks). The implemented models were based on the Deng et al work about multimodal affect analysis [17]. This is an incremental work of their refactorized code that adds two extra modalities that are the body skeleton and instantaneous emotions.\n",
        "\n",
        "Figure 2 shows the System Architecture of our proposed model. Our deep neural network model consists of three parts: (1) the sub-networks for every single modality; (2) the early fusion layer which concatenates four unimodal representations together; and (3) the final decision layer that estimates the sen- timent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_L200mkeniy",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper02.png)\n",
        "\n",
        "Fig. 2.Architecture model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY6CcwNUYy7R",
        "colab_type": "text"
      },
      "source": [
        "### Subnetworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ea8KkrY1AE",
        "colab_type": "text"
      },
      "source": [
        "There are 10 subnetworks, some of them are spatio- temporal RNN based on sequences of 20 samples, and some others are DNN  of variable vector size. The features from face and body are composed of hand- crafted and deep features, both of them are based on RNN approaches. In- stantaneous emotions and audio features have an RNN subnetwork and a DNN subnetwork. The two text subnetworks are only based on DNN approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTEEgVoEY4cS",
        "colab_type": "text"
      },
      "source": [
        "### Early fusion layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed6CZ--HY6XI",
        "colab_type": "text"
      },
      "source": [
        "This part concatenates four unimodal representa- tions together, in specific face related features and body related features are joined. The concatenated features from a single video clip are further fed into     an LSTM layer with 64 hidden units followed by a dense layer with 256 hidden neurons for temporal modeling. The audio and emotion features are then fed     into a fully connected layer with 256 units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3scHHdeqY-En",
        "colab_type": "text"
      },
      "source": [
        "### Fusion and Decision Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B8S1DsCY_d4",
        "colab_type": "text"
      },
      "source": [
        "We combine cues from the four modalities using early fusion strategy. The aggregated feature vector is fully connected to     a two-layer neural network with 1024 hidden units and a single output neuron, activated by softmax for the classification tasks. For the regression tasks we use MSE as the loss function for joint training and a CCC loss (1 - CCC) for further refinement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALzhilyaZFV5",
        "colab_type": "text"
      },
      "source": [
        "## Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-2qLkAZZG_P",
        "colab_type": "text"
      },
      "source": [
        "The MultiAffect framework is designed to perform classification and regression tasks. Depending on the performed task the platform is adjusted to display meaningful evaluations. The classification task gives accuracy metrics for train- ing, validation, and testing sets. In the case of a regression task, the framework computes the concordance correlation coefficients (CCC) that describes how well Jupyter notebooks are capable to display graphs and plots within the docu- ment. There are some python libraries such as matlibplot that generates plots and display them behind a notebook cell. The categorization tasks generate two plots where training and validation lines are displayed for the accuracy and loss metrics. The confusion matrix is also displayed for the categorization task and depends on the number of classes defined. The regression tasks display a scatter plot that shows how the predicted and gold standard labels are related. The higher the concentration of points on a 45-degree line, the better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTY1OoJe18n",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x7BJevkZye4",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k45-FnM1Z2K5",
        "colab_type": "text"
      },
      "source": [
        "Our aim with MultiAffect framework is to generate reproducible research for complex video analysis tasks. In order to test its generalizability, we performed affect recognition and video action recognition tasks. The video action and the affect recognition models were classification and regression tasks respectively. The main goal was to perform both actions without changing the code and by just setting the configuration variables. Another goal was to achieve average scores of existing work. Since we are implementing a generic vanilla version of the algorithms, we do not expect to have outstanding results. Our framework focuses on simplicity but it is flexible enough to integrate complex state-of-the- art algorithms with ease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slZtTFnaZ5_q",
        "colab_type": "text"
      },
      "source": [
        "## Video Action Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MaE6iSGZ9Ta",
        "colab_type": "text"
      },
      "source": [
        "The video action recognition project that we chose to test the framework was  a blooper recognition model for monologue videos. A blooper is a short clip from a film or video production, containing a mistake made by a person on screen. Monologue videos have fixed conditions, such as one person at a time and a fixed camera position. Detecting bloopers is not a trivial task even with  a sophisticated tool. The blooper recognition task can be implemented by using video action recognition approaches from multimodal features. Bloopers cannot be identified from instantaneous shots, so long-term video analysis is needed. The conditions required for performing this categorization task are ideal use case of MultiAffect.\n",
        "\n",
        "The database used to perform this task was Blooper DB. The Blooper DB is a long-term multimodal corpus for blooper recognition. It is constructed by picking out the videos that contain video bloopers from Youtube videos using keywords like ‘bloopers’, ‘green screen’, etc. The videos have multiple resolutions and multiple languages. The dataset is split into training, validation and testing sets. There are 464 videos in the training set, 66 videos in the validation set, and 66 videos in the testing set. Each video clip is annotated by categorical labels, 0 (no blooper) and 1 (blooper). Each video clip lasts between 1 to 3 seconds. The dataset is stratified and has an equal number of samples per each category. Figure 3 shows some examples of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYaFLD6OfBGK",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper04.png)\n",
        "\n",
        "Fig. 3.Bloopers DB samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa9zexKpfJDq",
        "colab_type": "text"
      },
      "source": [
        "We trained and evaluated the multimodal network on Blooper DB. The model was trained for at most 300 epochs. To prevent overfitting, we applied an early- stopping policy with 20 epochs patience, which means to stop training after the validation loss does not drop for 20 epochs, and we deployed dropout strategy with ratio 0.5 for each fully connected layer. The learning rate was 1e-3.\n",
        "\n",
        "**Unimodal Approach**\n",
        "\n",
        "We first evaluated the performance of a model trained with a single modality. All the available framework modalities were used except text. We decided to do not evaluate text since our approach intents to be language agnostic. Table 1 shows the result of the unimodal approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY32W38wfaJC",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper05.png)\n",
        "\n",
        "Table 1.Unimodal and multimodal results for the video action recognition task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRRokY83fYlk",
        "colab_type": "text"
      },
      "source": [
        "**Multimodal Approach**\n",
        "\n",
        "We titled the fusion of all the evaluated modalities as the quadmodal network. The quadmodel contains the Face Handcrafted, Face Deep. Body Handcrafted, Body Deep, Audio General, and Emotion Temporal features. We trained the quadmodal network by using the concatenated multimodal features. Figure 4 shows the training performance metrics. With respect to fusion strategies. We compared the early and late feature fusion strategies in Table 2. The results demonstrated that learning benefits more from early fused representation. Table 1 shows the comparison of the unimodal and multimodal performances. We also compared the fusion of the top 3 accuracy unimodal models with the quadmodal, but quadmodal outperformed the model containing face and audio features only. The quadmodal model has better performance than any of the tested unimodal and multimodal models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDuuiu_kf5YO",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper08.png)\n",
        "\n",
        "Table 2.Early versus late fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFmKEgQfmFQ",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper06.png)\n",
        "\n",
        "Fig. 4.Accuracy and loss plots of the Quadmodal model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48hyBpHfjNA",
        "colab_type": "text"
      },
      "source": [
        "Then we evaluated the confusion matrices for the quadmodel to evaluate how was the performance per each category over the different sets. Figure 5 shows the confusion matrices of each set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw2WwxtYfujw",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper07.png)\n",
        "\n",
        "Fig. 5.Confusion matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOblEDl-ftRz",
        "colab_type": "text"
      },
      "source": [
        "The results obtained were outstanding, achieving perfect scores in the valida- tion scenario. We validate that MultiAffect is a viable framework for performing action recognition tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-XuPrcxaFZi",
        "colab_type": "text"
      },
      "source": [
        "## Affect recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9pkbA1waL0X",
        "colab_type": "text"
      },
      "source": [
        "In order to test how a regression task can be performed by using MultiAffect framework, we chose an affect recognition task that predicts valence and arousal as dimensional values. We chose to test our framework by performing the task required for an emotion recognition challenge. The 2018 One-Minute Gradual- Emotion Recognition (OMG-Emotion) challenge, which was held in conjunction with the IEEE World Congress on Computational Intelligence, encouraged par- ticipants to address long-term emotion recognition by integrating cues from mul- tiple modalities, including facial expression, audio, and language. Intuitively, a multimodal inference network should be able to leverage information from each modality and their correlations to improve recognition over that achievable by a single modality network.\n",
        "\n",
        "The challenge provided the One-Minute Gradual-Emotional Behavior dataset that contains utterance-level videos from long-term emotional behaviors such as “monologues”, “auditions”, “dialogues” and “emotional scenes”. The dataset consists of 5288 (train: 2442, validation: 617, test: 2229) segments from YouTube videos of about 1-minute each. We used the OMG Emotion dataset to recognize emotions in long-term natural utterances. Figure 6 shows some dataset samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkKK7vMxgDsM",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper09.png)\n",
        "\n",
        "Fig. 6. OMG-Emotion dataset samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxao7K9OgC_C",
        "colab_type": "text"
      },
      "source": [
        "We configured the framework similar to the  video  action  recognition  ap-  proach. The only parameters that were changed were the  database  source,  label files, and the task type was switched from ‘category’ to ‘arousal,valence’. We performed unimodal and multimodal approaches to evaluate the framework per- formance.\n",
        "\n",
        "**Unimodal Approach**\n",
        "\n",
        "The modalities used for these tasks were the same that the used for the action recognition task. Table 3 shows the obtained results for this task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-8AvXC8gRcC",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper10.png)\n",
        "\n",
        "Table 3.Unimodal results for the affect recognition task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18NFLXU8gTg0",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper11.png)\n",
        "\n",
        "Fig. 7.Concordance Correlation Coefficient plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7uShfg5gbXo",
        "colab_type": "text"
      },
      "source": [
        "**Multimodal Approach**\n",
        "\n",
        "The same multimodal approaches applied to the action recognition model were applied to this task. Table 4 shows the obtained results. Figure 7 shows an example of how the evaluation data is represented in a plot.\n",
        "\n",
        "The results obtained were average according to the OMG-Emotion challenge results [3]. This can be explained by the basic version of each algorithm. We prove that our approach can perform regression tasks over affectve tasks and obtain competitive results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD4fg7DCgWI9",
        "colab_type": "text"
      },
      "source": [
        "![alt text](http://www.carlostoxtli.com/files/paper12.png)\n",
        "\n",
        "Table 4.Multimodal  early  fusion  results.  F=Face  A=Audio  T=Text  B=BodyE=Emotions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGTGERgqPDFX",
        "colab_type": "text"
      },
      "source": [
        "# Perform experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOItW6OvYje_",
        "colab_type": "text"
      },
      "source": [
        "## MultiAffect Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqY1HmPQIl2l",
        "colab_type": "text"
      },
      "source": [
        "### Environment variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_CwZwBODhb",
        "colab_type": "code",
        "outputId": "b77e79d7-a05e-4548-85e4-4926082d73f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "import os\n",
        "local_env = 'COLAB_GPU' in os.environ\n",
        "will_install_local_pip = False\n",
        "\n",
        "will_download_files = True\n",
        "\n",
        "will_extract_features = False\n",
        "will_save_in_drive = False\n",
        "will_download_openface = True\n",
        "will_download_opensmile = True\n",
        "\n",
        "will_train_model = True\n",
        "will_use_computed_features = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11 µs, sys: 1 µs, total: 12 µs\n",
            "Wall time: 13.8 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPJXlI8wIkwT",
        "colab_type": "code",
        "outputId": "37e28313-a45f-44fe-ccc3-4d20b1b83a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "if will_download_files:\n",
        "  !pip install gdown\n",
        "if will_extract_features and will_save_in_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "Collecting filelock (from gdown)\n",
            "Collecting six (from gdown)\n",
            "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting requests (from gdown)\n",
            "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
            "Collecting tqdm (from gdown)\n",
            "  Using cached https://files.pythonhosted.org/packages/45/af/685bf3ce889ea191f3b916557f5677cc95a5e87b2fa120d74b5dd6d049d0/tqdm-4.32.1-py2.py3-none-any.whl\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->gdown)\n",
            "  Using cached https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl\n",
            "Collecting certifi>=2017.4.17 (from requests->gdown)\n",
            "  Using cached https://files.pythonhosted.org/packages/60/75/f692a584e85b7eaba0e03827b3d51f45f571c2e793dd731e598828d380aa/certifi-2019.3.9-py2.py3-none-any.whl\n",
            "Collecting chardet<3.1.0,>=3.0.2 (from requests->gdown)\n",
            "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
            "Collecting idna<2.9,>=2.5 (from requests->gdown)\n",
            "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
            "Installing collected packages: filelock, six, urllib3, certifi, chardet, idna, requests, tqdm, gdown\n",
            "Successfully installed certifi-2019.3.9 chardet-3.0.4 filelock-3.0.12 gdown-3.8.1 idna-2.8 requests-2.22.0 six-1.12.0 tqdm-4.32.1 urllib3-1.25.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3S3ZoOEuSDL",
        "colab_type": "text"
      },
      "source": [
        "### Download files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U7_X5L0c6C4",
        "colab_type": "code",
        "outputId": "0ea68ba7-dc14-4058-c5ab-7bdcd735beb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "import os\n",
        "\n",
        "if will_download_files:\n",
        "  if will_extract_features:\n",
        "    if not os.path.exists('pretrained_models'):\n",
        "      !gdown https://drive.google.com/uc?id=1izx3yeH6hYivMnFqE66kLBI3y4tk9H5L\n",
        "      !unzip pretrained_models.zip\n",
        "    if not os.path.exists('OMG_Emotion_videos'):\n",
        "      !echo You must provide your database\n",
        "      !gdown https://drive.google.com/uc?id=1ForNowO_yF0-imtbNJmDHIET07bDydTl\n",
        "      !unzip OMG_Emotion_videos.zip\n",
        "  if will_train_model:\n",
        "    if will_use_computed_features:\n",
        "      if not os.path.exists('features'):\n",
        "        !gdown https://drive.google.com/uc?id=1VudAMlcXmGNJegWrEp6yjxGOO1BSCpbi\n",
        "        !unzip features.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 23 µs, sys: 3 µs, total: 26 µs\n",
            "Wall time: 24.1 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUKwa9xEZh_j",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJfzsICJYTpf",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRxeCiwTYO6B",
        "colab_type": "code",
        "outputId": "751e511c-ecc5-4f92-a90b-976d2844f7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "%%time\n",
        "if local_env:\n",
        "  if will_install_local_pip:\n",
        "    !pip install tabulate Pillow asposebarcode asposestorage automium-web indras-net keras keras-vggface matplotlib nltk numpy opencv-contrib-python opencv-contrib-python-headless opencv-python opencv-python-headless pandas pyskyscanner roomai scikit-learn scikit-learn-runnr scipy tensorflow-gpu theano tqdm transferflow GPUtil numba moviepy\n",
        "else:\n",
        "  !pip install GPUtil\n",
        "  !pip install opencv-python --upgrade\n",
        "!pip install git+https://github.com/rcmalli/keras-vggface.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\r\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "Collecting opencv-python\n",
            "  Using cached https://files.pythonhosted.org/packages/77/30/36c3f0644fa9f42d92f079b972e990a5874c1fc2b2c0e9656eb88bb8d6dc/opencv_python-4.1.0.25-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting numpy>=1.11.1 (from opencv-python)\n",
            "  Using cached https://files.pythonhosted.org/packages/1f/c7/198496417c9c2f6226616cff7dedf2115a4f4d0276613bab842ec8ac1e23/numpy-1.16.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Installing collected packages: numpy, opencv-python\n",
            "Successfully installed numpy-1.16.4 opencv-python-4.1.0.25\n",
            "Collecting git+https://github.com/rcmalli/keras-vggface.git\n",
            "  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-U57hEL-build\n",
            "Collecting numpy>=1.9.1 (from keras-vggface==0.5)\n",
            "  Using cached https://files.pythonhosted.org/packages/1f/c7/198496417c9c2f6226616cff7dedf2115a4f4d0276613bab842ec8ac1e23/numpy-1.16.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting scipy>=0.14 (from keras-vggface==0.5)\n",
            "  Using cached https://files.pythonhosted.org/packages/81/39/f1457091d0a45a84a2bd7815e2cf6bd45d4fe240728e9ed567cbb17c8abe/scipy-1.2.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting h5py (from keras-vggface==0.5)\n",
            "  Using cached https://files.pythonhosted.org/packages/53/08/27e4e9a369321862ffdce80ff1770553e9daec65d98befb2e14e7478b698/h5py-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting pillow (from keras-vggface==0.5)\n",
            "  Using cached https://files.pythonhosted.org/packages/b6/4b/5adc1109908266554fb978154c797c7d71aba43dd15508d8c1565648f6bc/Pillow-6.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
            "Collecting keras (from keras-vggface==0.5)\n",
            "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
            "Collecting six>=1.9.0 (from keras-vggface==0.5)\n",
            "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting pyyaml (from keras-vggface==0.5)\n",
            "Collecting keras-applications>=1.0.6 (from keras->keras-vggface==0.5)\n",
            "Collecting keras-preprocessing>=1.0.5 (from keras->keras-vggface==0.5)\n",
            "  Using cached https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl\n",
            "Installing collected packages: numpy, scipy, six, h5py, pillow, keras-applications, keras-preprocessing, pyyaml, keras, keras-vggface\n",
            "  Running setup.py install for keras-vggface ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25hSuccessfully installed h5py-2.9.0 keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.0.9 keras-vggface-0.5 numpy-1.16.4 pillow-6.0.0 pyyaml-5.1 scipy-1.2.1 six-1.12.0\n",
            "CPU times: user 322 ms, sys: 68.7 ms, total: 391 ms\n",
            "Wall time: 12.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTjwqYL6Yadb",
        "colab_type": "code",
        "outputId": "d6321ef4-0377-4805-a13a-6ae237eb8999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import gc\n",
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import sys\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import uuid\n",
        "import GPUtil\n",
        "import pickle\n",
        "import shutil\n",
        "import random\n",
        "import tabulate\n",
        "import warnings\n",
        "import argparse\n",
        "import itertools\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from numba import cuda\n",
        "import tensorflow as tf\n",
        "from random import randint\n",
        "from subprocess import call\n",
        "import matplotlib.pyplot as plt\n",
        "#from __future__ import print_function\n",
        "from moviepy.editor import VideoFileClip\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "#from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras.engine import Layer, InputSpec\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils.vis_utils import plot_model, model_to_dot\n",
        "from keras import metrics, initializers, regularizers, constraints\n",
        "from keras.layers import Input, Dense, Dropout, concatenate, Activation\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.layers.pooling import  AveragePooling1D, GlobalAveragePooling1D\n",
        "from keras.applications.resnet50 import preprocess_input as res_preprocess\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9pXFY2tpd3f",
        "colab_type": "text"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TvSi6k-p9fk",
        "colab_type": "text"
      },
      "source": [
        "### Feature Extraction Pre requisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbFyY0TdptHw",
        "colab_type": "text"
      },
      "source": [
        "#### OpenFace Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zh5et1cSmYS",
        "colab_type": "code",
        "outputId": "b6056747-d503-40ae-f0bc-c4d83c828645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "if will_extract_features and will_download_openface:\n",
        "  !apt install autoconf automake autopoint autotools-dev debhelper dh-autoreconf dh-strip-nondeterminism file gettext gettext-base gir1.2-atk-1.0 gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-pango-1.0 intltool-debian libarchive-cpio-perl libarchive-zip-perl libatk1.0-dev libcairo-script-interpreter2 libcairo2-dev libfile-stripnondeterminism-perl libgail-common libgail18 libgdk-pixbuf2.0-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libgtk2.0-dev libmagic-mgc libmagic1 libmail-sendmail-perl libpango1.0-dev libpangoxft-1.0-0 libpixman-1-dev libsigsegv2 libsys-hostname-long-perl libtimedate-perl libtool libxcb-shm0-dev libxcomposite-dev libxcursor-dev libxinerama-dev libxml2-utils libxrandr-dev m4 po-debconf x11proto-composite-dev x11proto-randr-dev x11proto-xinerama-dev\n",
        "  !gdown https://drive.google.com/uc?id=1lPgWBQPb40YNQuHZ3hGFIzkUG3stCZAy\n",
        "  !unzip OpenFace.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22 µs, sys: 15 µs, total: 37 µs\n",
            "Wall time: 8.11 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMGu1voJpvma",
        "colab_type": "text"
      },
      "source": [
        "#### OpenSmile Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbLvGTjyqB6E",
        "colab_type": "code",
        "outputId": "ce2d2768-81a8-4a88-8335-abf6794bf1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "if will_extract_features and will_download_opensmile:\n",
        "  !git clone https://github.com/naxingyu/opensmile.git\n",
        "  %cd opensmile\n",
        "  !sed -i '117s/(char)/(unsigned char)/g' src/include/core/vectorTransform.hpp\n",
        "  !./buildStandalone.sh\n",
        "  %cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21 µs, sys: 15 µs, total: 36 µs\n",
            "Wall time: 5.96 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2GbxQAMqWmz",
        "colab_type": "text"
      },
      "source": [
        "### Feature Extraction Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72pho7vtqZnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sudo apt install ffmpeg\n",
        "#python features_from_file.py --model ../experiment/models/fusion_early__face_feature__face_visual__audio_feature.hdf5 --clip te1_3.mp4\n",
        "#python features_from_file.py --batch ../Videos/Test  --types audio_rnn\n",
        "#python features_from_file.py --sets ../Videos  --types audio_rnn\n",
        "#python features_from_file.py --clip ../Videos/Test/video_1/utterance_51.mp4 --types audio_rnn --no_pkl\n",
        "#python features_from_file.py --video mientras.mp4 --model ../experiment/models/fusion_early__face_feature__face_visual__audio_feature.hdf5\n",
        "\n",
        "defaults = {\n",
        "'frame_size': (224, 224),\n",
        "'vector_size': 40\n",
        "}\n",
        "defaults['point_center'] = (int(defaults['frame_size'][0]/2), int(defaults['frame_size'][1]/2))\n",
        "models = []\n",
        "extractor_obj = None\n",
        "\n",
        "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
        "                \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"MidHip\": 8, \"RHip\": 9,\n",
        "                \"RKnee\": 10, \"RAnkle\": 11, \"LHip\": 12, \"LKnee\": 13, \"LAnkle\": 14,\n",
        "                \"REye\": 15, \"LEye\": 16, \"REar\": 17, \"LEar\": 18, \"LBigToe\": 19,\n",
        "                \"LSmallToe\": 20, \"LHeel\": 21, \"RBigToe\": 22, \"RSmallToe\": 23,\n",
        "                \"RHeel\": 24, \"Background\": 25 }\n",
        "\n",
        "vector_colors = [(0, 0, 0),\n",
        "              (0, 0, 255),\n",
        "              (0, 255, 0),\n",
        "              (255, 0, 0),\n",
        "              (255, 255, 0),\n",
        "              (0, 255, 255),\n",
        "              (255, 0, 255),\n",
        "              (127, 0, 0),\n",
        "              (0, 127, 0),\n",
        "              (0, 0, 127),\n",
        "              (127, 127, 0),\n",
        "              (0, 127, 127),\n",
        "              (127, 0, 127),\n",
        "              (255, 127, 0),\n",
        "              (127, 255, 0),\n",
        "              (0, 127, 255),\n",
        "              (0, 255, 127),\n",
        "              (255, 0, 127),\n",
        "              (127, 0, 255)]\n",
        "\n",
        "class Extractor():\n",
        "    def __init__(self, layer = 'fc6', model = 'vgg16'):\n",
        "        self.model = model\n",
        "        self.layer = layer\n",
        "        # Get model with pretrained weights. model: vgg16, resnet50\n",
        "        vgg_model = VGGFace(model)\n",
        "        if self.model == 'vgg16':\n",
        "            # We'll extract features at the fc6 layer\n",
        "            self.model = Model(\n",
        "                    inputs=vgg_model.input,\n",
        "                    outputs=vgg_model.get_layer(layer).output\n",
        "                    )\n",
        "        elif self.model == 'resnet50':\n",
        "            resent_out = vgg_model.get_layer(layer).output\n",
        "            out = Flatten(name='flatten')(resent_out)\n",
        "            self.model = Model(\n",
        "                    inputs=vgg_model.input,\n",
        "                    outputs=out\n",
        "                    )\n",
        "        \n",
        "    def extract(self, image_path):\n",
        "        img = image.load_img(image_path, target_size=(224, 224))\n",
        "        x = image.img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0) \n",
        "        if self.model == 'vgg16':\n",
        "            x = vgg_preprocess(x)\n",
        "        elif self.model == 'resnet50':\n",
        "            x = res_preprocess(x)\n",
        "        # Get the prediction.\n",
        "        features = self.model.predict(x)\n",
        "        features = features[0]\n",
        "        return features\n",
        "\n",
        "    def dispose(self):\n",
        "        del self.model\n",
        "        gc.collect()\n",
        "        #K.clear_session()\n",
        "\n",
        "def get_seq_length_indices(length):\n",
        "  if length < seq_length:\n",
        "    interval = list(range(length))\n",
        "    for i in range(seq_length-length):\n",
        "      interval.append(length-1)\n",
        "  else:\n",
        "    ratio = length//seq_length\n",
        "    init_value = length - 1\n",
        "    interval = [init_value]\n",
        "    for i in range(seq_length-1):\n",
        "      init_value -= ratio\n",
        "      interval.append(init_value)\n",
        "    interval.sort()\n",
        "  return interval\n",
        "\n",
        "def filter_list_to_seq_length(elements):\n",
        "    indices = get_seq_length_indices(len(elements))\n",
        "    return [elements[index] for index in indices]\n",
        "\n",
        "def get_video_length(filename):\n",
        "    try:\n",
        "        clip = VideoFileClip(filename)\n",
        "        length = clip.duration\n",
        "        clip.reader.close()\n",
        "        del clip\n",
        "        return length\n",
        "    except:\n",
        "        return 1\n",
        "\n",
        "def extract_faces(video_name, video_path):\n",
        "    #print('EXTRACTING FACES')\n",
        "    cur_dir = os.getcwd()\n",
        "    os.chdir(video_path)\n",
        "    cmd = OpenFace_Extractor_path + ' -f '+ video_name\n",
        "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr= subprocess.STDOUT, universal_newlines=True, close_fds=True, bufsize=-1)\n",
        "    out, err = process.communicate()\n",
        "    #print(out)\n",
        "    os.chdir(cur_dir)\n",
        "\n",
        "def extract_audio(video_file, audio_path, audio_file):\n",
        "    #print('\\nEXTRACTING AUDIO')\n",
        "    #process1 = subprocess.call(['ffmpeg', '-i', video_file, '-vn', audio_file]) # extract audio file from video file\"\"\"\n",
        "    video_length = get_video_length(video_file)\n",
        "    #print('VIDEO_LENGTH', video_length)\n",
        "    process = subprocess.Popen(['ffmpeg', '-i', video_file, '-vn', audio_file],stdout=subprocess.PIPE, stderr= subprocess.STDOUT, close_fds=True, bufsize=-1)\n",
        "    out, err = process.communicate()\n",
        "    audio_interval = video_length/seq_length # seconds\n",
        "    if audio_interval < 0.1:\n",
        "      audio_interval = 0.1\n",
        "    #print('INTERVAL', audio_interval)\n",
        "    audio_frames_path = os.path.join(audio_path, 'frames')\n",
        "    os.makedirs(audio_frames_path)\n",
        "    frame_files = os.path.join(audio_frames_path, 'out%04d.wav')\n",
        "    #process2 = subprocess.call(['ffmpeg', '-i', video_file, '-f','segment','-segment_time', str(audio_interval), frame_files])\n",
        "    process = subprocess.Popen(['ffmpeg', '-i', video_file, '-f','segment','-segment_time', str(audio_interval), frame_files],stdout=subprocess.PIPE, stderr= subprocess.STDOUT, close_fds=True, bufsize=-1)\n",
        "    out, err = process.communicate()\n",
        "\n",
        "def extract_audio_features(audio_file, audio_feature_file):\n",
        "    cmd = opensmile_script_path + ' -C ' + opensmile_conf + ' -I ' + audio_file +' -O '+ audio_feature_file\n",
        "    #print(cmd)\n",
        "    #subprocess.check_call(cmd.split(), stdout=subprocess.PIPE, stderr= subprocess.STDOUT)\n",
        "    process = subprocess.Popen(cmd.split(),stdout=subprocess.PIPE, stderr= subprocess.STDOUT, close_fds=True, bufsize=-1)\n",
        "    out, err = process.communicate()\n",
        "    #process.wait()\n",
        "    #print(out)\n",
        "\n",
        "def audio_features_integrity_check(features):\n",
        "    for i, feature in enumerate(features):\n",
        "        if type(feature) == str:\n",
        "            if i == 0:\n",
        "                features[i] = features[i + 1]\n",
        "            else:\n",
        "                features[i] = features[i - 1]\n",
        "    return features\n",
        "\n",
        "def extract_audio_rnn(audio_path):\n",
        "    output = []\n",
        "    audio_frames_path = os.path.join(audio_path, 'frames')\n",
        "    audio_features_path = os.path.join(audio_path, 'features')\n",
        "    if not os.path.exists(audio_features_path):\n",
        "        os.makedirs(audio_features_path)\n",
        "    audio_files = glob.glob(os.path.join(audio_frames_path, '*'))\n",
        "    #print('FILES', len(audio_files))\n",
        "    audio_files = filter_list_to_seq_length(audio_files)\n",
        "    #print('FILES_FIX', len(audio_files))\n",
        "    for i, audio_file in enumerate(audio_files):\n",
        "        audio_features_file = os.path.join(audio_features_path, '%s.txt' % i)\n",
        "        extract_audio_features(audio_file, audio_features_file)\n",
        "        features = extract_audio_features_from_txt(audio_features_file)\n",
        "        #print(features)\n",
        "        output.append(features)\n",
        "    output = audio_features_integrity_check(output)\n",
        "    return np.asarray(output)\n",
        "\n",
        "def clean_audio_data(data):\n",
        "    data = data.split(',')[1:] #'noname' deleted\n",
        "    data[-1] = data[-1][0:-1] #'\\n' deleted\n",
        "    length = len(data)\n",
        "    new_data = np.zeros(length)\n",
        "    for i, item in enumerate(data):\n",
        "        new_data[i] = float(item)\n",
        "    return new_data[0:1582]\n",
        "\n",
        "def extract_audio_features_from_txt(txt_file):\n",
        "    data = {}\n",
        "    file = open(txt_file, 'r')\n",
        "    while True:\n",
        "        line = file.readline()\n",
        "        if line:\n",
        "            if line.startswith('@data'):\n",
        "                line = file.readline()\n",
        "                line = file.readline()\n",
        "                data = line\n",
        "                if data: #sometimes , the data might be empty\n",
        "                    data = clean_audio_data(data)\n",
        "                    return data\n",
        "        else:\n",
        "            return data\n",
        "\n",
        "def load_extractor_once(layer, model):\n",
        "    global extractor_obj\n",
        "    if extractor_obj == None:\n",
        "        extractor_obj = Extractor(layer, model)\n",
        "    return extractor_obj\n",
        "\n",
        "def extract_face_features(video_path):\n",
        "    utter_csv = os.path.join(video_path, 'processed', 'vid.csv')        \n",
        "    return read_csv_return_face_feature(utter_csv)\n",
        "\n",
        "def extract_face_visual(video_path):\n",
        "    model = 'vgg16'\n",
        "    layer = 'fc6'\n",
        "    extractor = load_extractor_once(layer, model)\n",
        "    utter_csv = os.path.join(video_path, 'processed', 'vid.csv')\n",
        "    selected_frames = read_openface_csv(utter_csv)\n",
        "    if selected_frames == None:\n",
        "        print (\"No face detected in video:\", video_name, \"utterance_\", utter_index,\"Skipping...\")\n",
        "    else:\n",
        "        # intialization\n",
        "        features = np.zeros(4096)\n",
        "        sequence = []\n",
        "        for frame in sorted(selected_frames):\n",
        "            try:\n",
        "                features = extractor.extract(frame) \n",
        "            except:\n",
        "                print(\"Error extracting for \"+frame)\n",
        "                features = features\n",
        "            sequence.append(features)\n",
        "    #extractor.dispose()\n",
        "    utter_feature = np.asarray(sequence)\n",
        "    return utter_feature\n",
        "\n",
        "def float_a_list(list):\n",
        "    new_list = []\n",
        "    for item in list:\n",
        "        new_list.append(float(item))\n",
        "    return new_list\n",
        "\n",
        "def turn_frame_index_into_path (frame_list, parent_dir):\n",
        "    path = os.path.join(parent_dir, 'vid_aligned')\n",
        "    frame_path_list = []\n",
        "    for frame_index in frame_list:\n",
        "        frame_path = os.path.join(path, 'frame_det_00_'+'{0:06d}'.format(frame_index)+'.bmp')\n",
        "        # assume the file exists\n",
        "        frame_path_list.append(frame_path)\n",
        "    return frame_path_list\n",
        "\n",
        "def read_openface_csv(file_path):\n",
        "    \"\"\"\n",
        "    frame, face_id, timestamp, confidence, success, gaze_0_x, ...\n",
        "    from an utternace frames, \n",
        "    return a list of frame index\n",
        "    \"\"\"\n",
        "    parent_dir = os.path.dirname(file_path)\n",
        "    selected_frames = []\n",
        "    df = pd.read_csv(file_path)\n",
        "    confidence_index = [ i for i, s in enumerate(df[df.columns[4]]) if float(s) == 1]\n",
        "    if len(confidence_index) == 0 :\n",
        "        # no face detected \n",
        "        return None\n",
        "    length = len(confidence_index)\n",
        "    taken_index = []\n",
        "    if length<seq_length:\n",
        "        strate = 'repeat_final'\n",
        "        final_index = confidence_index[-1]\n",
        "        taken_index = confidence_index\n",
        "    else:\n",
        "        strate = 'equal_interval'\n",
        "        interval = length//seq_length \n",
        "        for i in range(seq_length):\n",
        "            taken_index.append(confidence_index[i*interval])\n",
        "    reader = csv.reader(open(file_path,'r'))\n",
        "    next(reader)\n",
        "    for index, row in enumerate(reader):\n",
        "        if index in taken_index:\n",
        "            if (strate == 'repeat_final') and (index == final_index):\n",
        "                for i in range(seq_length - length +1 ):\n",
        "                    selected_frames.append(int(row[0]))\n",
        "            else:\n",
        "                selected_frames.append(int(row[0]))\n",
        "    assert len(selected_frames) == seq_length\n",
        "\n",
        "    return turn_frame_index_into_path (selected_frames,parent_dir= parent_dir)\n",
        "\n",
        "def read_csv_return_face_feature(file_path):\n",
        "    \"\"\"\n",
        "    frame, face_id, timestamp, confidence, success, gaze_0_x, ...\n",
        "    from an utternace frames, \n",
        "    \"\"\"\n",
        "    data = []\n",
        "    df = pd.read_csv(file_path)\n",
        "    confidence_index = [ i for i, s in enumerate(df[df.columns[4]]) if float(s) == 1]\n",
        "    if len(confidence_index) == 0:\n",
        "        # no face detected \n",
        "        return None\n",
        "    length = len(confidence_index)\n",
        "    taken_index = []\n",
        "    if length<seq_length:\n",
        "        strate = 'repeat_final'\n",
        "        final_index = confidence_index[-1]\n",
        "        taken_index = confidence_index\n",
        "    else:\n",
        "        strate = 'equal_interval'\n",
        "        interval = length//seq_length \n",
        "        for i in range(seq_length):\n",
        "            taken_index.append(confidence_index[i*interval])\n",
        "    reader = csv.reader(open(file_path,'r'))\n",
        "    next(reader)\n",
        "    for index, row in enumerate(reader):\n",
        "        if index in taken_index:\n",
        "            if (strate == 'repeat_final') and (index == final_index):\n",
        "                for i in range(seq_length - length +1 ):\n",
        "                    data.append( float_a_list(row[5:]))\n",
        "            else:\n",
        "                data.append(float_a_list(row[5:]))\n",
        "    data = np.asarray(data)\n",
        "    assert data.shape[0] == seq_length \n",
        "    return data\n",
        "\n",
        "def get_next_point(point_end, point_central, vector_rate):\n",
        "    point_pivot = defaults['point_center']\n",
        "    x = int(((point_end[0] - point_central[0]) / vector_rate) + point_pivot[0])\n",
        "    y = int(((point_end[1] - point_central[1]) / vector_rate) + point_pivot[1])\n",
        "    return (x,y)\n",
        "\n",
        "def adjust_points(points):\n",
        "  #show_original(points)\n",
        "  coords = {}\n",
        "  frame = np.zeros([defaults['frame_size'][0], defaults['frame_size'][1], 3],dtype=np.uint8)\n",
        "  if points[BODY_PARTS[\"Neck\"]] is not None:\n",
        "    coords['Neck'] = defaults['point_center']\n",
        "    point_neck = point_central = points[BODY_PARTS[\"Neck\"]]\n",
        "    point_shoulder = None\n",
        "    if points[BODY_PARTS[\"LShoulder\"]] and points[BODY_PARTS[\"RShoulder\"]]:\n",
        "        if abs(points[BODY_PARTS[\"LShoulder\"]][0]-point_neck[0]) >= abs(points[BODY_PARTS[\"RShoulder\"]][0]-point_neck[0]):\n",
        "            point_shoulder = points[BODY_PARTS[\"LShoulder\"]]\n",
        "        else:\n",
        "            point_shoulder = points[BODY_PARTS[\"RShoulder\"]]\n",
        "    elif points[BODY_PARTS[\"LShoulder\"]]:\n",
        "        point_shoulder = points[BODY_PARTS[\"LShoulder\"]]\n",
        "    elif points[BODY_PARTS[\"RShoulder\"]]:\n",
        "        point_shoulder = points[BODY_PARTS[\"RShoulder\"]]\n",
        "    if point_shoulder is not None:\n",
        "        vector_size = math.hypot(point_shoulder[0] - point_neck[0], point_shoulder[1] - point_neck[1])\n",
        "        vector_rate = vector_size / defaults['vector_size']\n",
        "        if vector_rate == 0:\n",
        "           return frame\n",
        "        if points[BODY_PARTS[\"LShoulder\"]] is not None:\n",
        "            coords['LShoulder'] = get_next_point(points[BODY_PARTS[\"LShoulder\"]], point_central, vector_rate)\n",
        "        if points[BODY_PARTS[\"LElbow\"]] is not None:\n",
        "            coords['LElbow'] = get_next_point(points[BODY_PARTS[\"LElbow\"]], point_central, vector_rate)\n",
        "            if points[BODY_PARTS[\"LWrist\"]] is not None:\n",
        "                coords['LWrist'] = get_next_point(points[BODY_PARTS[\"LWrist\"]], point_central, vector_rate)\n",
        "        if points[BODY_PARTS[\"RShoulder\"]] is not None:\n",
        "            coords['RShoulder'] = get_next_point(points[BODY_PARTS[\"RShoulder\"]], point_central, vector_rate)\n",
        "        if points[BODY_PARTS[\"RElbow\"]] is not None:\n",
        "            coords['RElbow'] = get_next_point(points[BODY_PARTS[\"RElbow\"]], point_central, vector_rate)\n",
        "            if points[BODY_PARTS[\"RWrist\"]] is not None:\n",
        "                coords['RWrist'] = get_next_point(points[BODY_PARTS[\"RWrist\"]], point_central, vector_rate)\n",
        "        if points[BODY_PARTS[\"Nose\"]] is not None:\n",
        "            coords['Nose'] = get_next_point(points[BODY_PARTS[\"Nose\"]], point_central, vector_rate)\n",
        "            if points[BODY_PARTS[\"LEye\"]] is not None:\n",
        "                coords['LEye'] = get_next_point(points[BODY_PARTS[\"LEye\"]], point_central, vector_rate)\n",
        "                if points[BODY_PARTS[\"LEar\"]] is not None:\n",
        "                    coords['LEar'] = get_next_point(points[BODY_PARTS[\"LEar\"]], point_central, vector_rate)\n",
        "            if points[BODY_PARTS[\"REye\"]] is not None:\n",
        "                coords['REye'] = get_next_point(points[BODY_PARTS[\"REye\"]], point_central, vector_rate)\n",
        "                if points[BODY_PARTS[\"REar\"]] is not None:\n",
        "                    coords['REar'] = get_next_point(points[BODY_PARTS[\"REar\"]], point_central, vector_rate)\n",
        "        if points[BODY_PARTS[\"MidHip\"]] is not None:\n",
        "            coords['MidHip'] = get_next_point(points[BODY_PARTS[\"MidHip\"]], point_central, vector_rate)\n",
        "            if points[BODY_PARTS[\"LHip\"]] is not None:\n",
        "                coords['LHip'] = get_next_point(points[BODY_PARTS[\"LHip\"]], point_central, vector_rate)\n",
        "            if points[BODY_PARTS[\"RHip\"]] is not None:\n",
        "                coords['RHip'] = get_next_point(points[BODY_PARTS[\"RHip\"]], point_central, vector_rate)\n",
        "\n",
        "        # print(coords)\n",
        "        if 'Neck' in coords:\n",
        "            if 'MidHip' in coords:\n",
        "                #print(coords['Neck'], coords['MidHip'], vector_colors[0], 3)\n",
        "                cv2.line(frame, coords['Neck'], coords['MidHip'], vector_colors[0], 3)\n",
        "                if 'LHip' in coords:\n",
        "                    cv2.line(frame, coords['MidHip'], coords['LHip'], vector_colors[0], 3)\n",
        "                if 'RHip' in coords:\n",
        "                    cv2.line(frame, coords['MidHip'], coords['RHip'], vector_colors[0], 3)\n",
        "            if 'LShoulder' in coords:\n",
        "                cv2.line(frame, coords['Neck'], coords['LShoulder'], vector_colors[1], 3)\n",
        "                if 'LElbow' in coords:\n",
        "                    cv2.line(frame, coords['LShoulder'], coords['LElbow'], vector_colors[4], 3)\n",
        "                    if 'LWrist' in coords:\n",
        "                        cv2.line(frame, coords['LElbow'], coords['LWrist'], vector_colors[10], 3)\n",
        "            if 'RShoulder' in coords:\n",
        "                cv2.line(frame, coords['Neck'], coords['RShoulder'], vector_colors[2], 3)\n",
        "                if 'RElbow' in coords:\n",
        "                    cv2.line(frame, coords['RShoulder'], coords['RElbow'], vector_colors[5], 3)\n",
        "                    if 'RWrist' in coords:\n",
        "                        cv2.line(frame, coords['RElbow'], coords['RWrist'], vector_colors[11], 3)\n",
        "            if 'Nose' in coords:\n",
        "                cv2.line(frame, coords['Neck'], coords['Nose'], vector_colors[3], 3)\n",
        "                if 'LEye' in coords:\n",
        "                    cv2.line(frame, coords['Nose'], coords['LEye'], vector_colors[6], 3)\n",
        "                    if 'LEar' in coords:\n",
        "                        cv2.line(frame, coords['LEye'], coords['LEar'], vector_colors[8], 3)\n",
        "                if 'REye' in coords:\n",
        "                    cv2.line(frame, coords['Nose'], coords['REye'], vector_colors[7], 3)\n",
        "                    if 'REar' in coords:\n",
        "                        cv2.line(frame, coords['REye'], coords['REar'], vector_colors[9], 3)\n",
        "\n",
        "        for coord in coords:\n",
        "            pass\n",
        "            #cv2.ellipse(frame, coords[coord], (3, 3), 0, 0, 360, (127, 127, 127), cv2.FILLED)\n",
        "\n",
        "        # cv2.imshow('OpenPose using OpenCV', frame)\n",
        "        # while cv2.waitKey(1) < 0:\n",
        "        #   pass\n",
        "  return frame\n",
        "\n",
        "def get_angle(points, nameA, nameB):\n",
        "  if points[BODY_PARTS[nameA]] is not None and points[BODY_PARTS[nameB]] is not None:\n",
        "    pointA = points[BODY_PARTS[nameA]]\n",
        "    pointB = points[BODY_PARTS[nameB]]\n",
        "    myradians = math.atan2(pointA[0]-pointB[0], pointA[1]-pointB[1])\n",
        "    return abs(math.degrees(myradians))\n",
        "  return None\n",
        "\n",
        "def get_body_features(points):\n",
        "  features = np.zeros(11)\n",
        "  features[0] = 0 if points[BODY_PARTS[\"Nose\"]] is None else 1\n",
        "  features[1] = 0 if points[BODY_PARTS[\"Neck\"]] is None else 1\n",
        "  features[2] = 0 if points[BODY_PARTS[\"LShoulder\"]] is None else 1\n",
        "  features[3] = 0 if points[BODY_PARTS[\"LElbow\"]] is None else 1\n",
        "  features[4] = 0 if points[BODY_PARTS[\"RShoulder\"]] is None else 1\n",
        "  features[5] = 0 if points[BODY_PARTS[\"RElbow\"]] is None else 1\n",
        "  feature = get_angle(points, \"Neck\", \"Nose\")\n",
        "  features[6] = 0 if feature is None else feature / 90\n",
        "  feature = get_angle(points, \"Neck\", \"LShoulder\")\n",
        "  features[7] = 0 if feature is None else (feature - 90) / 90\n",
        "  feature = get_angle(points, \"Neck\", \"RShoulder\")\n",
        "  features[8] = 0 if feature is None else (feature - 90) / 90\n",
        "  feature = get_angle(points, \"LShoulder\", \"LElbow\")\n",
        "  features[9] = 0 if feature is None else (180 - feature) / 180\n",
        "  feature = get_angle(points, \"RShoulder\", \"RElbow\")\n",
        "  features[10] = 0 if feature is None else (180 - feature) / 180\n",
        "  return features\n",
        "\n",
        "def process_image(cap, net, show=False, store=None, images=False, calc_features=True, save_images=True):\n",
        "  thr = 0.1\n",
        "  inWidth = 368\n",
        "  inHeight = 368\n",
        "  inScale = 0.003922\n",
        "  #while cv2.waitKey(1) < 0:\n",
        "  output = []\n",
        "  j = 0\n",
        "  length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  #print('LENGTH', length)\n",
        "  n = 0\n",
        "  cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "  while True:\n",
        "    n += 1\n",
        "    hasFrame, frame = cap.read()\n",
        "    if not hasFrame:\n",
        "      break\n",
        "  length = n - 1\n",
        "  #print('REAL', length)  \n",
        "  cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "  if length < seq_length:\n",
        "    interval = list(range(length))\n",
        "    for i in range(seq_length-length):\n",
        "      interval.append(length-1)\n",
        "  else:\n",
        "    ratio = length//seq_length\n",
        "    init_value = length - 1\n",
        "    interval = [init_value]\n",
        "    for i in range(seq_length-1):\n",
        "      init_value -= ratio\n",
        "      interval.append(init_value)\n",
        "    interval.sort()\n",
        "\n",
        "  for index in interval:\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
        "    hasFrame, frame = cap.read()\n",
        "    if not hasFrame:\n",
        "        break\n",
        "        #print('BREAK')\n",
        "        #cv2.waitKey()\n",
        "        #break\n",
        "    frameWidth = frame.shape[1]\n",
        "    frameHeight = frame.shape[0]\n",
        "    #image = cv2.resize(frame, (224, 224))\n",
        "    #inp = cv2.dnn.blobFromImage(image,1, (224, 224), (104, 117, 123), swapRB=True)\n",
        "    inp = cv2.dnn.blobFromImage(frame, inScale, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n",
        "    net.setInput(inp)\n",
        "    out = net.forward()\n",
        "\n",
        "    assert(len(BODY_PARTS) <= out.shape[1])\n",
        "    points = []\n",
        "    for k in range(len(BODY_PARTS)):\n",
        "        # Slice heatmap of corresponging body's part.\n",
        "        heatMap = out[0, k, :, :]\n",
        "\n",
        "        # Originally, we try to find all the local maximums. To simplify a sample\n",
        "        # we just find a global one. However only a single pose at the same time\n",
        "        # could be detected this way.\n",
        "        _, conf, _, point = cv2.minMaxLoc(heatMap)\n",
        "        x = (frameWidth * point[0]) / out.shape[3]\n",
        "        y = (frameHeight * point[1]) / out.shape[2]\n",
        "\n",
        "        # Add a point if it's confidence is higher than threshold.\n",
        "        points.append((int(x), int(y)) if conf > thr else None)\n",
        "    if calc_features:\n",
        "      features = get_body_features(points)\n",
        "    else:\n",
        "      features = points\n",
        "      #a = np.asarray(out)\n",
        "      #features = a.flatten()\n",
        "    #print(features)\n",
        "    output.append(features)\n",
        "    if images:\n",
        "      frame = adjust_points(points)\n",
        "\n",
        "      t, _ = net.getPerfProfile()\n",
        "      freq = cv2.getTickFrequency() / 1000\n",
        "      #cv2.putText(frame, '%.2fms' % (t / freq), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
        "      if store is not None:\n",
        "        if save_images:\n",
        "          cv2.imwrite(os.path.join(store, str(j)+'.png'), frame)\n",
        "      if show:\n",
        "        cv2.imshow('OpenPose using OpenCV', frame)\n",
        "        while cv2.waitKey(1) < 0:\n",
        "          pass\n",
        "    j += 1\n",
        "    #print(index)\n",
        "  return output\n",
        "\n",
        "def extract_body(video_file, body_visual_path, body_feature_file):\n",
        "  #print('EXTRACTING BODY')\n",
        "  net = cv2.dnn.readNet(body_proto, body_model)\n",
        "  net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  points = process_image(cap, net, store=body_visual_path, images=True, calc_features=False, save_images=True)\n",
        "  json.dump(points, open(body_feature_file, 'w'))\n",
        "\n",
        "def extract_body_features(body_path):\n",
        "  features = []\n",
        "  points = json.load(open(body_path, 'r'))\n",
        "  for point in points:\n",
        "    features.append(get_body_features(point))\n",
        "  return np.asarray(features)\n",
        "\n",
        "def extract_body_visual(body_path):\n",
        "    model = 'vgg16'\n",
        "    layer = 'fc6'\n",
        "    extractor = load_extractor_once(layer, model)\n",
        "    files = glob.glob(os.path.join(body_path, '*'))\n",
        "    sequence = []\n",
        "    for file in files:\n",
        "        features = np.zeros(4096)        \n",
        "        try:\n",
        "            features = extractor.extract(file) \n",
        "        except:\n",
        "            print(\"Error extracting for \" + file)\n",
        "        sequence.append(features)\n",
        "    #extractor.dispose()\n",
        "    return np.asarray(sequence)\n",
        "\n",
        "def init_models_once():\n",
        "  global models\n",
        "  if len(models) == 0:\n",
        "      # https://github.com/priya-dwivedi/face_and_emotion_detection\n",
        "      models.append(load_model(emotion_path_1))\n",
        "      # https://github.com/petercunha/Emotion\n",
        "      models.append(load_model(emotion_path_2))\n",
        "      #https://github.com/thoughtworksarts/EmoPy/blob/master/EmoPy/models/conv_model_0256.hdf5\n",
        "      models.append(load_model(emotion_path_3))\n",
        "      #https://github.com/thoughtworksarts/EmoPy/blob/master/EmoPy/models/conv_model_145.hdf5\n",
        "      models.append(load_model(emotion_path_4))\n",
        "      #https://github.com/oarriaga/face_classification/blob/master/trained_models/emotion_models/fer2013_mini_XCEPTION.107-0.66.hdf5\n",
        "      models.append(load_model(emotion_path_5))\n",
        "      #https://github.com/oarriaga/face_classification/blob/master/trained_models/emotion_models/simple_CNN.985-0.66.hdf5\n",
        "      models.append(load_model(emotion_path_6))\n",
        "\n",
        "def preprocess1(image):\n",
        "  face_image  = cv2.imread(image)\n",
        "  face_image = cv2.resize(face_image, (48,48))\n",
        "  face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
        "  face_image = np.reshape(face_image, [1, face_image.shape[0], face_image.shape[1], 1])\n",
        "  return face_image\n",
        "\n",
        "def preprocess2(image):\n",
        "  face_image  = cv2.imread(image)\n",
        "  face_image = cv2.resize(face_image, (64,64))\n",
        "  face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n",
        "  face_image = np.reshape(face_image, [1, face_image.shape[0], face_image.shape[1], 1])\n",
        "  return face_image\n",
        "\n",
        "def get_emotion_features(image):\n",
        "  global models\n",
        "  features = np.array([])\n",
        "  aggregated = []\n",
        "  emotion_dict= {'Angry': 0, 'Sad': 5, 'Neutral': 4, 'Disgust': 1, 'Surprise': 6, 'Fear': 2, 'Happy': 3}\n",
        "\n",
        "  img1 = preprocess1(image)\n",
        "  img2 = preprocess2(image)\n",
        "\n",
        "  out1 = models[0].predict(img1)\n",
        "  features = np.append(features, out1)\n",
        "  out2 = models[1].predict(img2)\n",
        "  features = np.append(features, out2)\n",
        "  out3 = models[2].predict(img1)\n",
        "  out3 /= 100\n",
        "  features = np.append(features, out3)\n",
        "  out4 = models[3].predict(img1)\n",
        "  out4 /= 100\n",
        "  features = np.append(features, out4)\n",
        "  out5 = models[4].predict(img2)\n",
        "  features = np.append(features, out5)\n",
        "  out6 = models[5].predict(img1)\n",
        "  features = np.append(features, out6)\n",
        "  return features\n",
        "\n",
        "def extract_emotion_global(features):\n",
        "  output = []\n",
        "  num_models = 5\n",
        "  num_features = 7\n",
        "  values = np.zeros(num_features*num_models)\n",
        "  for feature in features:\n",
        "    for i in range(num_models):\n",
        "      num_init = i * num_features\n",
        "      num_end = num_init + num_features\n",
        "      index = np.argmax(feature[num_init:num_end])\n",
        "      values[num_init+index] += 1\n",
        "  for i in range(num_models):\n",
        "    num_init = i * num_features\n",
        "    num_end = num_init + num_features\n",
        "    subset = values[num_init:num_end]\n",
        "    ref = sum(subset)\n",
        "    subset = [float(i)/ref for i in subset]\n",
        "    output += subset\n",
        "  return output\n",
        "\n",
        "def extract_emotion_features(video_path):\n",
        "  #print('EXTRACTING EMOTIONS')\n",
        "  output = []\n",
        "  init_models_once()\n",
        "  images = glob.glob(os.path.join(video_path, 'processed', 'vid_aligned', '*'))\n",
        "  num_images = len(images)\n",
        "  increment = 1\n",
        "  if num_images > seq_length:\n",
        "    increment = num_images // seq_length\n",
        "  for i in range(seq_length):\n",
        "    index = i * increment\n",
        "    if index >= num_images:\n",
        "      index = num_images - 1\n",
        "    image = images[index]\n",
        "    features = get_emotion_features(image)\n",
        "    output.append(features)\n",
        "  return np.asarray(output)\n",
        "\n",
        "def delete_directory(vid_path):\n",
        "    shutil.rmtree(vid_path)\n",
        "\n",
        "def get_scalers(model_types):\n",
        "    scalers = {}\n",
        "    for model_type in model_types:\n",
        "        scalers[model_type] = get_scaler(model_type)\n",
        "    return scalers\n",
        "\n",
        "def get_scaler(model_type=None):\n",
        "    scaler = MinMaxScaler()\n",
        "    if model_type is not None:\n",
        "        name  = '_'.join([s.capitalize() for s in model_type.split('_')])\n",
        "        pkl_path = os.path.join(data_path, name + '.pkl')\n",
        "        features = pickle.load(open(pkl_path, 'rb'))\n",
        "        feature = np.asarray(features['Train']['video_1'].values())\n",
        "        #print(feature[0].shape)\n",
        "        if len(feature[0].shape) == 2:\n",
        "            feature_dim = feature[0].shape[1]\n",
        "            unrolled_f = feature.reshape((-1,feature_dim))\n",
        "        else:\n",
        "            unrolled_f  = feature\n",
        "        #print('SCALER-unenrolled', unrolled_f.shape)\n",
        "        scaler.fit_transform(unrolled_f)\n",
        "    return scaler\n",
        "\n",
        "def normalize_feature(feature, model_type, scaler=None):\n",
        "    if scaler is None:\n",
        "        scaler = get_scaler(model_type)\n",
        "    #print('feature', feature.shape)\n",
        "    if len(feature.shape) == 2:\n",
        "        time_steps = feature.shape[0]\n",
        "        feature_dim = feature.shape[1]\n",
        "        unrolled_f = np.asarray(feature).reshape((-1,feature_dim))\n",
        "        #print('unrolled_f', unrolled_f.shape)\n",
        "        scaled_f = scaler.transform(unrolled_f)\n",
        "        scaled_f = scaled_f.reshape((-1, time_steps, feature_dim))\n",
        "    else:\n",
        "        unrolled_f  = np.asarray([feature])\n",
        "        scaled_f = scaler.transform(unrolled_f)\n",
        "    #print('scaled_f', scaled_f.shape)\n",
        "    return scaled_f\n",
        "\n",
        "def normalize_features(features, model_type, scaler=None):\n",
        "    if scaler is None:\n",
        "        scaler = get_scaler(model_type)\n",
        "    #print('features[0]', features[0].shape)\n",
        "    if len(features[0].shape) == 2:\n",
        "        time_steps = features[0].shape[0]\n",
        "        feature_dim = features[0].shape[1]\n",
        "        unrolled_f = np.asarray(features).reshape((-1,feature_dim))\n",
        "        #print('unrolled_f', unrolled_f.shape)\n",
        "        scaled_f = scaler.transform(unrolled_f)\n",
        "        scaled_f = scaled_f.reshape((-1, time_steps, feature_dim))\n",
        "    else:\n",
        "        unrolled_f  = np.asarray(features)\n",
        "        scaled_f = scaler.transform(unrolled_f)\n",
        "    #print('scaled_f', scaled_f.shape)\n",
        "    return scaled_f\n",
        "\n",
        "def get_model_components(model_path, features):\n",
        "  components = []\n",
        "  model_file_name = model_path.split('/')[-1]\n",
        "  model_name = model_file_name.split('.')[0]\n",
        "  model_components = model_name.split('__')\n",
        "  for model_component in model_components:\n",
        "    if model_component in features.keys():\n",
        "        components.append(model_component)\n",
        "  return components\n",
        "\n",
        "def get_types_from_model(model_path):\n",
        "  components = []\n",
        "  model_file_name = model_path.split('/')[-1]\n",
        "  model_name = model_file_name.split('.')[0]\n",
        "  model_components = model_name.split('__')\n",
        "  for model_component in model_components:\n",
        "    if model_component in all_types:\n",
        "        components.append(model_component)\n",
        "  return components\n",
        "\n",
        "def get_X(features, model_components, scalers=None, normalize=True):\n",
        "  X = []\n",
        "  for model_component in model_components:\n",
        "    feature = features[model_component]\n",
        "    scaler = None\n",
        "    if scalers is not None:\n",
        "        if model_component in scalers:\n",
        "            scaler = scalers[model_component]\n",
        "    if normalize:\n",
        "        feature = normalize_feature(feature, model_component, scaler)\n",
        "    X.append(feature)\n",
        "  return X\n",
        "\n",
        "def get_X_all(features, model_components, scalers=None, normalize=True):\n",
        "  start = True\n",
        "  X_all = []\n",
        "  for model_component in model_components:\n",
        "    #print('model_component', model_component)\n",
        "    values = [feature[model_component] for feature in features]\n",
        "    scaler = None\n",
        "    if scalers is not None:\n",
        "        if model_component in scalers:\n",
        "            scaler = scalers[model_component]\n",
        "    if normalize:\n",
        "        values = normalize_features(values, model_component, scaler)\n",
        "    for i,value in enumerate(values):\n",
        "        if start:\n",
        "           X_all.append([])\n",
        "        X_all[i].append([value])\n",
        "    start = False\n",
        "  return X_all\n",
        "\n",
        "def get_clip_features(file_path, types=None, store_pkl=True):\n",
        "    features = []\n",
        "    filename = file_path.split('/')[-1]\n",
        "    pkl_path = filename + '.pkl'\n",
        "    if os.path.exists(pkl_path):\n",
        "        features = pickle.load(open(pkl_path, 'rb'))\n",
        "    else:\n",
        "        features = extract_features(file_path, types)\n",
        "        if store_pkl:\n",
        "            pickle.dump(features, open(pkl_path, 'wb'), 2)\n",
        "    return features\n",
        "\n",
        "def get_video_features(file_path, types=None, store_pkl=True):\n",
        "    features = []\n",
        "    filename = file_path.split('/')[-1]\n",
        "    pkl_path = filename + '.all.pkl'\n",
        "    if os.path.exists(pkl_path):\n",
        "        features = pickle.load(open(pkl_path, 'rb'))\n",
        "    else:\n",
        "        features = extract_video_features(file_path, types)\n",
        "        if store_pkl:\n",
        "            pickle.dump(features, open(pkl_path, 'wb'), 2)\n",
        "    return features\n",
        "\n",
        "def predict_values(features, model_path, loaded_model=None):\n",
        "  predictions = []\n",
        "  if len(features) > 0:\n",
        "      model_components = get_model_components(model_path, features[0])\n",
        "      scalers = get_scalers(model_components)\n",
        "      X_all = get_X_all(features, model_components, scalers)\n",
        "      if loaded_model is not None:\n",
        "        model = loaded_model\n",
        "      else:\n",
        "        model = load_model(model_path)\n",
        "      model.summary()\n",
        "      #model.layers.pop()\n",
        "      #model.summary()\n",
        "      #for feature in features:\n",
        "      for X in tqdm(X_all):\n",
        "          #X = get_X(feature, model_components, scalers)\n",
        "          #model.summary()\n",
        "          #plot_model(model)\n",
        "          #for i in X:\n",
        "          #  print(i.shape)\n",
        "          y = model.predict(X)\n",
        "          #print(y)\n",
        "          predictions.append(y)\n",
        "  return predictions\n",
        "\n",
        "def split_video_into_clips(file_path, output_path, parts_per_second, chunk_length):\n",
        "    #print('SPLITTING FILES')\n",
        "    time_segment = 1.0 / parts_per_second\n",
        "    time_init = 0.0\n",
        "    segments_path = os.path.join(output_path, 'segments')\n",
        "    chunks_path = os.path.join(output_path, 'chunks')\n",
        "    if not os.path.exists(segments_path):\n",
        "        os.makedirs(segments_path)\n",
        "        os.makedirs(chunks_path)\n",
        "    files_path = os.path.join(segments_path, 'output%03d.mp4')\n",
        "    for i in range(parts_per_second):\n",
        "        tmp_video = os.path.join(output_path, 'tmp%s.mp4' % i)\n",
        "        cmd1 = 'ffmpeg -ss 00:00:0{:.3f} -noaccurate_seek -i {} -c copy -ss 00:00:0{:.3f} -reset_timestamps 1 {}'.format(time_init, file_path, time_init, tmp_video)\n",
        "        #subprocess.call(cmd1.split())\n",
        "        #print(cmd1)\n",
        "        process = subprocess.Popen(cmd1.split(),stdout=subprocess.PIPE, stderr= subprocess.STDOUT, close_fds=True, bufsize=-1)\n",
        "        out, err = process.communicate()\n",
        "        cmd2 = 'ffmpeg -i {} -c copy -segment_time 00:00:{:02d} -f segment -reset_timestamps 1 {}'.format(tmp_video, chunk_length, files_path)\n",
        "        #subprocess.call(cmd2.split())\n",
        "        #print(cmd2)\n",
        "        process = subprocess.Popen(cmd2.split(),stdout=subprocess.PIPE, stderr= subprocess.STDOUT, close_fds=True, bufsize=-1)\n",
        "        out, err = process.communicate()\n",
        "        files = glob.glob(os.path.join(segments_path, '*'))\n",
        "        files.sort()\n",
        "        time_init += time_segment\n",
        "        segment_num = i\n",
        "        os.remove(tmp_video)\n",
        "        for file in files:\n",
        "            chunks_file = os.path.join(chunks_path, 'chunk_{:05d}.mp4'.format(segment_num))\n",
        "            shutil.move(file, chunks_file)\n",
        "            segment_num += parts_per_second\n",
        "    chunks = glob.glob(os.path.join(chunks_path, '*'))\n",
        "    return chunks\n",
        "\n",
        "def extract_video_features(file_path, types=None):\n",
        "    features = []\n",
        "    uid = str(uuid.uuid1())\n",
        "    vid_path = os.path.join(videos_path, uid)\n",
        "    #print('PATH', vid_path)\n",
        "    if not os.path.exists(vid_path):\n",
        "        os.makedirs(vid_path)\n",
        "    clip_paths = split_video_into_clips(file_path, vid_path, 2, 1)\n",
        "    for clip_path in tqdm(clip_paths):\n",
        "        feature = extract_features(clip_path, types)\n",
        "        features.append(feature)\n",
        "        free_memory()\n",
        "    delete_directory(vid_path)\n",
        "    return features\n",
        "\n",
        "def free_memory(mem_models=None):\n",
        "    #GPUtil.showUtilization()\n",
        "    GPUs = GPUtil.getGPUs()\n",
        "    if len(GPUs) > 0:\n",
        "        pass\n",
        "        #print(GPUs[0].memoryTotal)\n",
        "        #print(GPUs[0].memoryUsed)\n",
        "        #print(GPUs[0].memoryFree)\n",
        "    pass\n",
        "    # if mem_models is not None:\n",
        "    #     for model in mem_models:\n",
        "    #         del model\n",
        "    # K.clear_session()\n",
        "    # for i in range(3):\n",
        "    #     gc.collect()\n",
        "    #     time.sleep(0.1)\n",
        "    #cuda.select_device(0)\n",
        "    #cuda.close()\n",
        "    #K.get_session().close()\n",
        "    #cfg = K.tf.ConfigProto()\n",
        "    #cfg.gpu_options.allow_growth = True\n",
        "    #K.set_session(K.tf.Session(config=cfg))\n",
        "    #print('CLEANNING MEMORY')\n",
        "    #time.sleep(1)\n",
        "\n",
        "def mem_setup():\n",
        "    cfg = K.tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
        "    #session_config = tf.ConfigProto(log_device_placement=False, allow_soft_placement=True)    \n",
        "    # please do not use the totality of the GPU memory\n",
        "    cfg.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
        "    K.set_session(K.tf.Session(config=cfg))\n",
        "\n",
        "def bin_values(predictions, index=1):\n",
        "    values = []\n",
        "    for prediction in predictions:\n",
        "        value = 0\n",
        "        if prediction[0][index] == 1:\n",
        "            value = 2\n",
        "        elif prediction[0][index] > 0.999995:\n",
        "            value = 1\n",
        "        values.append(value)\n",
        "    return values\n",
        "\n",
        "def get_time_intervals(n, clips_per_second, start_from=0.0):\n",
        "    step = 1.0 / clips_per_second\n",
        "    times = []\n",
        "    time_value = start_from\n",
        "    for i in range(n):\n",
        "        times.append(time_value)\n",
        "        time_value += step\n",
        "    return times  \n",
        "\n",
        "def analyze_results(results, clips_per_second, plot=False):\n",
        "    output = [[]]\n",
        "    times = get_time_intervals(len(results), clips_per_second)\n",
        "    if plot:\n",
        "        plt.plot(results)\n",
        "        plt.ylabel('Blooper Index')\n",
        "        plt.show()\n",
        "    bin_size = 3\n",
        "    values = []\n",
        "    for i in range(0, len(results)-bin_size):\n",
        "        values.append(np.sum(results[i:i+bin_size]))\n",
        "    times = get_time_intervals(len(values), clips_per_second)\n",
        "    if plot:\n",
        "        plt.plot(values)\n",
        "        plt.ylabel('Blooper Dispersion')\n",
        "        plt.show()\n",
        "    top3 = sorted(set(values), reverse=True)[:3]\n",
        "    batch_size = 5\n",
        "    scores = []\n",
        "    for i in range(0, len(values)-batch_size):\n",
        "        count = 0\n",
        "        for j in range(batch_size):\n",
        "            if values[i+j] in top3:\n",
        "                count += 1.0\n",
        "        scores.append(count/batch_size)\n",
        "    highest = []\n",
        "    for i, score in enumerate(scores):\n",
        "        if score >= 0.8:\n",
        "            highest.append(i + 1)\n",
        "    print('Predicted points')\n",
        "    print(highest)\n",
        "    if len(highest) > 0:\n",
        "        ranges = []\n",
        "        cur_range = [highest[0]]\n",
        "        for i in range(len(highest)-1):\n",
        "            if (highest[i+1]-highest[i]) == 1:\n",
        "                cur_range.append(highest[i+1])\n",
        "            else:\n",
        "                ranges.append(cur_range)\n",
        "                cur_range = [highest[i+1]]\n",
        "        ranges.append(cur_range)\n",
        "        for i, rang in enumerate(ranges):\n",
        "            if len(rang) == 1:\n",
        "                del ranges[i]\n",
        "        output = ranges\n",
        "        print('Predicted ranges')\n",
        "        print(ranges)\n",
        "    return output\n",
        "\n",
        "def predict_and_analyze(features, model_path, clips_per_second, loaded_model=None):\n",
        "    predictions = predict_values(features, model_path, loaded_model=loaded_model)\n",
        "    results = bin_values(predictions)\n",
        "    return analyze_results(results, clips_per_second, plot=False)\n",
        "\n",
        "def store_sets_features(sets_dir, types, states):\n",
        "    for state in states:\n",
        "        set_dir = os.path.join(sets_dir, state)\n",
        "        features = get_batch_features(set_dir, types)\n",
        "        features_per_type = get_features_per_type(features, types)\n",
        "        for type_name in types:\n",
        "            capitalized_name = get_capitalized_name(type_name)\n",
        "            set_name = '%s_%s.pkl' % (capitalized_name, state)\n",
        "            pickle.dump(features_per_type[type_name], open(set_name, 'wb'), 2)\n",
        "    for type_name in types:\n",
        "        capitalized_name = get_capitalized_name(type_name)\n",
        "        join_files(capitalized_name)\n",
        "\n",
        "def join_files(name):\n",
        "    path_train = '%s_Train.pkl' % name\n",
        "    path_validation = '%s_Validation.pkl' % name\n",
        "    if os.path.exists(path_train) and os.path.exists(path_validation):\n",
        "        output = {}\n",
        "        path_file = '%s.pkl' % name\n",
        "        output['Train'] = pickle.load(open(path_train, 'rb'))\n",
        "        output['Validation'] = pickle.load(open(path_validation, 'rb'))\n",
        "        pickle.dump(output, open(path_file, 'wb'), 2)\n",
        "\n",
        "def get_capitalized_name(name):\n",
        "    return name.replace('_', ' ').title().replace(' ', '_')\n",
        "\n",
        "def get_features_per_type(features, types):\n",
        "    output = {}\n",
        "    for type_name in types:\n",
        "        output[type_name] = {}\n",
        "        for video in features:\n",
        "            output[type_name][video] = {}\n",
        "            for uttr in features[video]:\n",
        "                output[type_name][video][uttr] = features[video][uttr][type_name]\n",
        "    return output\n",
        "  \n",
        "def get_batch_features_from_file(csv_file, files_path, types):\n",
        "  batch_dict = {}\n",
        "  reader = csv.DictReader(open(csv_file, 'r'))\n",
        "  for row in tqdm(reader):\n",
        "    video = row['video']\n",
        "    if video not in batch_dict:\n",
        "      batch_dict[video] = {}\n",
        "    utterance = row['utterance']\n",
        "    file_path = os.path.join(files_path, video, 'video', utterance)\n",
        "    if os.path.exists(file_path):\n",
        "      uttr_index = utterance.split('.')[0].split('_')[1]\n",
        "      batch_dict[video][uttr_index] = get_clip_features(file_path, types, store_pkl=False)\n",
        "  return batch_dict\n",
        "  \n",
        "def get_batch_features(batch_dir, types):\n",
        "  batch_dict = {}\n",
        "  videos = glob.glob(os.path.join(batch_dir, '*'))\n",
        "  for video in videos:\n",
        "    video_name = video.split('/')[-1]\n",
        "    if video_name not in batch_dict:\n",
        "      batch_dict[video_name] = {}\n",
        "    utterance_videos = glob.glob(os.path.join(video, '*.mp4'))\n",
        "    for uttr in tqdm(utterance_videos):\n",
        "      #print(uttr)\n",
        "      uttr_index = uttr.split('/')[-1].split('.')[0].split('_')[1]\n",
        "      batch_dict[video_name][uttr_index] = get_clip_features(uttr, types, store_pkl=False)\n",
        "  return batch_dict\n",
        "\n",
        "def from_file_save_batch_pickles(csv_file, features, types, store_pkl=False, output='./'):\n",
        "    batch_name = csv_file.split('/')[-1].split('.')[0].title()\n",
        "    if store_pkl:\n",
        "        batch_file = os.path.join(output, '%s.pkl' % batch_name)\n",
        "        pickle.dump(features, open(batch_file, 'wb'), 2)\n",
        "    features_per_type = get_features_per_type(features, types)\n",
        "    for type_name in types:\n",
        "        capitalized_name = get_capitalized_name(type_name)\n",
        "        batch_file = os.path.join(output, '%s_%s.pkl' % (capitalized_name, batch_name))\n",
        "        pickle.dump(features_per_type[type_name], open(batch_file, 'wb'), 2)\n",
        "\n",
        "def save_batch_pickles(batch_dir, features, types, store_pkl=True):\n",
        "    batch_name = batch_dir.replace('../', '').replace('/','_')\n",
        "    if store_pkl:\n",
        "        batch_file = '%s.pkl' % batch_name\n",
        "        pickle.dump(features, open(batch_file, 'wb'), 2)\n",
        "    features_per_type = get_features_per_type(features, types)\n",
        "    for type_name in types:\n",
        "        capitalized_name = get_capitalized_name(type_name)\n",
        "        batch_file = '%s_%s.pkl' % (capitalized_name, batch_name)\n",
        "        pickle.dump(features_per_type[type_name], open(batch_file, 'wb'), 2)\n",
        "\n",
        "def extract_features(filename, types=None):\n",
        "  features = {}\n",
        "  if types is None:\n",
        "    types = all_types\n",
        "  if os.path.exists(filename):\n",
        "    uid = str(uuid.uuid1())\n",
        "    vid_path = os.path.join(videos_path, uid)\n",
        "    audio_path = os.path.join(vid_path, 'audio')\n",
        "    body_path = os.path.join(vid_path, 'body')\n",
        "    body_visual_path = os.path.join(body_path, 'skelethon')\n",
        "    os.makedirs(vid_path)\n",
        "    os.makedirs(audio_path)\n",
        "    os.makedirs(body_visual_path)\n",
        "    video_name = 'vid.mp4'\n",
        "    video_file = os.path.join(vid_path, video_name)\n",
        "    audio_file = os.path.join(audio_path, 'audio.wav')\n",
        "    audio_feature_file = os.path.join(audio_path, 'audio.txt')\n",
        "    body_feature_file = os.path.join(body_path, 'body.json')\n",
        "    shutil.copyfile(filename, video_file)\n",
        "    if 'audio_feature' in types or 'audio_rnn' in types:\n",
        "        extract_audio(video_file, audio_path, audio_file)\n",
        "    if 'audio_feature' in types:\n",
        "        extract_audio_features(audio_file, audio_feature_file)\n",
        "        features['audio_feature'] = extract_audio_features_from_txt(audio_feature_file)\n",
        "    if 'audio_rnn' in types:\n",
        "        features['audio_rnn'] = extract_audio_rnn(audio_path)\n",
        "        #print('AUDIO_RNN', features['audio_rnn'].shape)\n",
        "    if 'face_feature' in types or 'face_visual' in types or 'emotion_feature' in types or 'emotion_global' in types:\n",
        "        extract_faces(video_name, vid_path)\n",
        "    if 'face_feature' in types:\n",
        "        features['face_feature'] = extract_face_features(vid_path)\n",
        "    if 'body_feature' in types or 'body_visual' in types:\n",
        "        extract_body(video_file, body_visual_path, body_feature_file)\n",
        "    if 'body_feature' in types:\n",
        "        features['body_feature'] = extract_body_features(body_feature_file)\n",
        "        #print(features['body_feature'].shape, features['body_visual'].shape)\n",
        "    if 'emotion_feature' in types:\n",
        "        features['emotion_feature'] = extract_emotion_features(vid_path)\n",
        "    if 'emotion_global' in types:\n",
        "        if 'emotion_feature' in types:\n",
        "            emotion_features = features['emotion_feature']\n",
        "        else:\n",
        "            emotion_features = extract_emotion_features(vid_path)\n",
        "        features['emotion_global'] = extract_emotion_global(emotion_features)\n",
        "        #print('emotion_global', features['emotion_global'])\n",
        "    if 'face_visual' in types:\n",
        "        features['face_visual'] = extract_face_visual(vid_path)\n",
        "    if 'body_visual' in types:\n",
        "        features['body_visual'] = extract_body_visual(body_visual_path)\n",
        "    if 'face_feature' in types and 'face_visual' in types:\n",
        "        features['face_fusion'] = np.concatenate((features['face_feature'], features['face_visual']), axis = -1)\n",
        "    if 'body_feature' in types and 'body_visual' in types:\n",
        "        features['body_fusion'] = np.concatenate((features['body_feature'], features['body_visual']), axis = -1)\n",
        "    #print(features)\n",
        "    delete_directory(vid_path)\n",
        "  else:\n",
        "    print('Provide a valid filename.')\n",
        "  return features\n",
        "\n",
        "def run(args):\n",
        "  model_path = args['model']\n",
        "  clips_per_second = args['clips_per_second']\n",
        "  clip_length = args['clip_length']\n",
        "  features = []\n",
        "  store_model = not args['no_pkl']\n",
        "  types = all_types\n",
        "  states = all_states\n",
        "  if args['types'] is not None:\n",
        "    types = args['types'].split(',')\n",
        "  if args['states'] is not None:\n",
        "    states = args['states'].split(',')\n",
        "  if args['join'] is not None:\n",
        "    join_files(args['join'])\n",
        "  if args['sets'] is not None:\n",
        "    store_sets_features(args['sets'], types, states)\n",
        "  if args['file'] is not None:\n",
        "    features = get_batch_features_from_file(args['file'], args['files_path'], types)\n",
        "    from_file_save_batch_pickles(args['file'], features, types, output=args['output'])\n",
        "  elif args['batch'] is not None:\n",
        "    features = get_batch_features(args['batch'], types)\n",
        "    save_batch_pickles(args['batch'], features, types)\n",
        "  else:\n",
        "    if model_path is not None:\n",
        "      types = get_types_from_model(model_path)\n",
        "    if args['clip'] is not None:\n",
        "      features = [get_clip_features(args['clip'], types, store_model)]\n",
        "      print(features)\n",
        "    elif args['video'] is not None:\n",
        "      features = get_video_features(args['video'], types, store_model)\n",
        "    elif args['features'] is not None:\n",
        "      if os.path.exists(args['features']):\n",
        "        features = pickle.load(open(args['features'], 'rb'))\n",
        "    if model_path is not None:\n",
        "      predictions = predict_values(features, model_path)\n",
        "      results = bin_values(predictions)\n",
        "      analyze_results(results, clips_per_second, plot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT_utVr4qaAT",
        "colab_type": "text"
      },
      "source": [
        "### Run Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ONCyoNqfwS",
        "colab_type": "code",
        "outputId": "fa633cf1-535a-4958-867d-bf299a4bcabe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "if local_env:\n",
        "  absolute_path = '/home/tox/projects/dl'\n",
        "  output_dir = './'\n",
        "else:\n",
        "  absolute_path = '/content'\n",
        "  output_dir = 'gdrive/My Drive/'\n",
        "\n",
        "seq_length = 20\n",
        "videos_path = 'vids'\n",
        "data_path = '../data'\n",
        "body_proto = 'pretrained_models/pose_body25.prototxt'\n",
        "body_model = 'pretrained_models/pose_body25.caffemodel'\n",
        "opensmile_script_path = 'opensmile/SMILExtract'\n",
        "opensmile_conf = 'opensmile/config/emobase2010.conf'\n",
        "OpenFace_Extractor_path = os.path.join(absolute_path, 'OpenFace/build/bin/FeatureExtraction')\n",
        "all_types = ['face_feature', 'face_visual', 'body_feature', 'body_visual', 'audio_feature', 'audio_rnn', 'emotion_feature', 'emotion_global']\n",
        "all_states = ['Validation', 'Test', 'Train']\n",
        "\n",
        "# https://github.com/priya-dwivedi/face_and_emotion_detection\n",
        "emotion_path_1 = \"pretrained_models/emotion1.hdf5\"\n",
        "# https://github.com/petercunha/Emotion\n",
        "emotion_path_2 = \"pretrained_models/emotion2.hdf5\"\n",
        "#https://github.com/thoughtworksarts/EmoPy/blob/master/EmoPy/models/conv_model_0256.hdf5\n",
        "emotion_path_3 = \"pretrained_models/emotion3.hdf5\"\n",
        "#https://github.com/thoughtworksarts/EmoPy/blob/master/EmoPy/models/conv_model_145.hdf5\n",
        "emotion_path_4 = \"pretrained_models/emotion4.hdf5\"\n",
        "#https://github.com/oarriaga/face_classification/blob/master/trained_models/emotion_models/fer2013_mini_XCEPTION.107-0.66.hdf5\n",
        "emotion_path_5 = \"pretrained_models/emotion5.hdf5\"\n",
        "#https://github.com/oarriaga/face_classification/blob/master/trained_models/emotion_models/simple_CNN.985-0.66.hdf5\n",
        "emotion_path_6 = \"pretrained_models/emotion6.hdf5\"\n",
        "\n",
        "args = {\n",
        "    'types': None,\n",
        "    'sets': None,\n",
        "    'file': None,\n",
        "    'files_path': None,\n",
        "    'batch': None,\n",
        "    'clip': None,\n",
        "    'video': None,\n",
        "    'features': None,\n",
        "    'model': None,\n",
        "    'clips_per_second': 2,\n",
        "    'clip_length': 2,\n",
        "    'no_pkl': False,\n",
        "    'join': None,\n",
        "    'states': None,\n",
        "    'output': output_dir\n",
        "}\n",
        "\n",
        "#args['sets'] = 'AutomEditor/videos'\n",
        "args['types'] = 'body_visual'\n",
        "args['file'] = \"features/test.csv\"\n",
        "args['files_path'] = \"videos\"\n",
        "\n",
        "if will_extract_features:\n",
        "  run(args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11 µs, sys: 2 µs, total: 13 µs\n",
            "Wall time: 11.9 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DRhaspoYgPt",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWXerdWU47ZF",
        "colab_type": "text"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVuA7NjKi6oS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = [\"Anger\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"]\n",
        "num_classes = len(classes)\n",
        "lstm_size = 64\n",
        "all_results = []\n",
        "all_features = {\n",
        "    'emotion_feature':{\n",
        "        'temporal': True,\n",
        "        'model': 'generic_rnn_model',\n",
        "        'size': 35\n",
        "    },\n",
        "    'emotion_global':{\n",
        "        'temporal': False,\n",
        "        'model': 'generic_dense_model',\n",
        "        'size': 35\n",
        "    },\n",
        "    'face_feature':{\n",
        "        'temporal': True,\n",
        "        'size': 709\n",
        "    },\n",
        "    'face_visual':{\n",
        "        'temporal': True,\n",
        "        'size': 4096\n",
        "    },\n",
        "    'face_fusion':{\n",
        "        'temporal': True,\n",
        "        'size': 4805\n",
        "    },\n",
        "    'body_feature':{\n",
        "        'temporal': True,\n",
        "        'size': 11\n",
        "    },\n",
        "    'body_visual':{\n",
        "        'temporal': True,\n",
        "        'size': 4096\n",
        "    },\n",
        "    'body_fusion':{\n",
        "        'temporal': True,\n",
        "        'size': 4107\n",
        "    },\n",
        "    'audio_feature':{\n",
        "        'temporal': False,\n",
        "        'size': 1582\n",
        "    },\n",
        "    'audio_rnn':{\n",
        "        'temporal': True,\n",
        "        'size': 1582\n",
        "    },\n",
        "    'word_feature':{\n",
        "        'temporal': False,\n",
        "        'size': 6\n",
        "    },\n",
        "    'word_mpqa':{\n",
        "        'temporal': False,\n",
        "        'size': 4\n",
        "    },\n",
        "    'word_fusion':{\n",
        "        'temporal': False,\n",
        "        'size': 10\n",
        "    }}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg0ov2ADYh5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse_metric(y_true, y_pred):\n",
        "    return mean_squared_error(y_true,y_pred)\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    from sklearn.metrics import f1_score\n",
        "    label = [0,1,2,3,4,5,6]\n",
        "    y_true =  np.argmax(y_true, axis=1)\n",
        "    y_pred =  np.argmax(y_pred, axis=1)\n",
        "    return f1_score(y_true,y_pred,labels=label,average=\"micro\")\n",
        "\n",
        "def ccc(y_true, y_pred):\n",
        "    true_mean = np.mean(y_true)\n",
        "    true_variance = np.var(y_true)\n",
        "    pred_mean = np.mean(y_pred)\n",
        "    pred_variance = np.var(y_pred)\n",
        "    rho,_ = pearsonr(y_pred,y_true)\n",
        "    std_predictions = np.std(y_pred)\n",
        "    std_gt = np.std(y_true)\n",
        "    ccc = 2 * rho * std_gt * std_predictions / (\n",
        "        std_predictions ** 2 + std_gt ** 2 +\n",
        "        (pred_mean - true_mean) ** 2)\n",
        "    return ccc, rho\n",
        "\n",
        "def calculateCCC(validationFile, modelOutputFile):\n",
        "    dataY = pandas.read_csv(validationFile, header=0, sep=\",\")\n",
        "\n",
        "    dataYPred = pandas.read_csv(modelOutputFile, header=0, sep=\",\")\n",
        "\n",
        "    dataYArousal = dataY[\"arousal\"]\n",
        "    dataYValence = dataY[\"valence\"]\n",
        "    dataYPredArousal = dataYPred[\"arousal\"]\n",
        "    dataYPredValence = dataYPred[\"valence\"]\n",
        "\n",
        "    arousalCCC, acor = ccc(dataYArousal, dataYPredArousal)\n",
        "    arousalmse = mse_metric(dataYArousal, dataYPredArousal)\n",
        "    valenceCCC, vcor = ccc(dataYValence, dataYPredValence)\n",
        "    valencemse = mse_metric(dataYValence, dataYPredValence)\n",
        "\n",
        "    print (\"Arousal CCC: \", arousalCCC)\n",
        "    print (\"Arousal Pearson Cor: \", acor)\n",
        "    print (\"Arousal MSE: \", arousalmse)\n",
        "    print (\"Valence CCC: \", valenceCCC)\n",
        "    print (\"Valence cor: \", vcor)\n",
        "    print (\"Valence MSE: \", valencemse)\n",
        "\n",
        "def ccc_loss(y_true, y_pred):\n",
        "    # using CCC as loss function\n",
        "    true_mean = K.mean(y_true)\n",
        "    pred_mean = K.mean(y_pred)\n",
        "    \n",
        "    std_predictions = K.std(y_pred)\n",
        "    std_gt = K.std(y_true)\n",
        "    \n",
        "    covariance = K.mean((y_true - true_mean)*(y_pred - pred_mean))\n",
        "    ccc = 2 * covariance / (\n",
        "    std_predictions ** 2 + std_gt ** 2 +\n",
        "    (pred_mean - true_mean) ** 2)\n",
        "\n",
        "    return 1 - ccc\n",
        "\n",
        "def ccc_metric(y_true, y_pred):\n",
        "    # using CCC as metric\n",
        "    true_mean = K.mean(y_true)\n",
        "    pred_mean = K.mean(y_pred)\n",
        "    \n",
        "    std_predictions = K.std(y_pred)\n",
        "    std_gt = K.std(y_true)\n",
        "    \n",
        "    covariance = K.mean((y_true - true_mean)*(y_pred - pred_mean))\n",
        "    ccc = 2 * covariance / (\n",
        "    std_predictions ** 2 + std_gt ** 2 +\n",
        "    (pred_mean - true_mean) ** 2)\n",
        "\n",
        "    return ccc\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true),axis = -1)\n",
        "\n",
        "def read_log(file_path):\n",
        "    # read log and return dictionary\n",
        "    # epoch,ccc_metric,loss,mean_squared_error,val_ccc_metric,val_loss,val_mean_squared_error\n",
        "    data = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        for index, line in enumerate(file):\n",
        "            if index ==0:\n",
        "                keys = line.rstrip('\\r\\n').split(',')\n",
        "                nums = len(keys)\n",
        "                for key in keys:\n",
        "                    data[key] = []\n",
        "            else:\n",
        "                numbers = line.rstrip('\\r\\n') .split(',')\n",
        "                for i in range(nums):\n",
        "                    data[keys[i]]  = float(numbers[i])\n",
        "    return data\n",
        "\n",
        "def plot_acc(history, model, index, show_plots=True, epochs=0):\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    filename = 'acc___%s_%d.png'%(model, epochs)\n",
        "    img_path = os.path.join('images')\n",
        "    if not os.path.exists(img_path):\n",
        "        os.makedirs(img_path)\n",
        "    name = os.path.join(img_path, filename)\n",
        "    plt.savefig(name)\n",
        "    if show_plots:\n",
        "        plt.show()\n",
        "    plt.clf()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    filename = 'loss___%s_%d.png'%(model, epochs)\n",
        "    name = os.path.join('images', filename)\n",
        "    plt.savefig(name)\n",
        "    if show_plots:\n",
        "        plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "def display_true_vs_pred(y_true, y_pred, log_path, task, model, show_plots=True, timestamp='', epochs=0):\n",
        "    #display the true label vs prediction using image\n",
        "    if not os.path.exists('images'):\n",
        "        os.mkdir('images')\n",
        "    #name = log_path.split('/')[-1].split('.')[0]\n",
        "    my_list = ['Validation','Train']\n",
        "    # draw the y_true and y_pred plot\n",
        "    for index,( y_t, y_p) in enumerate(zip(y_true, y_pred)):\n",
        "        name = os.path.join('images', (model+'___%d_%d_%s_'+task)%(epochs,index,timestamp))\n",
        "        plt.figure()\n",
        "        plt.scatter(y_p, y_t)\n",
        "        plt.xlabel('Prediction in {} set'.format(my_list[index]))\n",
        "        plt.ylabel('True label in {} set '.format(my_list[index]))\n",
        "        if task == 'arousal':\n",
        "            axes = plt.gca()\n",
        "            axes.set_xlim([0,1])\n",
        "            axes.set_ylim([0,1])\n",
        "            plt.text(0.1, 0.9,'CCC: %.3f\\nMSE: %.3f'%(ccc(y_t, y_p)[0],mse_metric(y_t, y_p)),\n",
        "         horizontalalignment='center',\n",
        "         verticalalignment='center')\n",
        "        elif task == 'valence':\n",
        "            axes = plt.gca()\n",
        "            axes.set_xlim([-1,1])\n",
        "            axes.set_ylim([-1,1])\n",
        "            plt.text(-0.8, 0.8,'CCC: %.3f\\nMSE: %.3f'%(ccc(y_t, y_p)[0],mse_metric(y_t, y_p)),\n",
        "         horizontalalignment='center',\n",
        "         verticalalignment='center')\n",
        "        elif task == 'category':\n",
        "            print(\"confusion matrix...\")\n",
        "            y_t =  np.argmax(y_t, axis=1)\n",
        "            y_p =  np.argmax(y_p, axis=1)\n",
        "            classes = [\"Anger\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"]\n",
        "            print(classes)\n",
        "            cm = confusion_matrix(y_t, y_p)\n",
        "            print(cm)\n",
        "            normalize = False\n",
        "            plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "            plt.title('Confusion Matrix')\n",
        "            plt.colorbar()\n",
        "            tick_marks = np.arange(len(classes))\n",
        "            plt.xticks(tick_marks, classes, rotation=45)\n",
        "            plt.yticks(tick_marks, classes)\n",
        "\n",
        "            fmt = '.2f' if normalize else 'd'\n",
        "            thresh = cm.max() / 2.\n",
        "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "                plt.text(j, i, format(cm[i, j], fmt),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "            plt.ylabel('True label')\n",
        "            plt.xlabel('Predicted label')\n",
        "            plt.tight_layout()\n",
        "        plt.savefig(name)\n",
        "        if show_plots:\n",
        "            plt.show()\n",
        "        plt.clf()\n",
        "            \n",
        "def display_true_vs_pred_emotion(y_true, y_pred, log_path, task, model, values, show_plots=True, timestamp='', epochs=0):\n",
        "    #display the true label vs prediction using image\n",
        "    if not os.path.exists('images'):\n",
        "        os.mkdir('images')\n",
        "    #name = log_path.split('/')[-1].split('.')[0]\n",
        "    my_list = ['Validation','Train','Test']\n",
        "    # draw the y_true and y_pred plot\n",
        "    for index,( y_t, y_p) in enumerate(zip(y_true, y_pred)):\n",
        "        name = os.path.join('images', 'cm-{}-{:.3f}-{}.png'.format(my_list[index], values[index], model))\n",
        "        print(\"confusion matrix...\")\n",
        "        y_t =  np.argmax(y_t, axis=1)\n",
        "        y_p =  np.argmax(y_p, axis=1)\n",
        "        print(classes)\n",
        "        cm = confusion_matrix(y_t, y_p)\n",
        "        print(cm)\n",
        "        normalize = False\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        plt.xticks(tick_marks, classes, rotation=45)\n",
        "        plt.yticks(tick_marks, classes)\n",
        "\n",
        "        fmt = '.2f' if normalize else 'd'\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, format(cm[i, j], fmt),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(name)\n",
        "        if show_plots:\n",
        "            plt.show()\n",
        "        plt.clf()\n",
        "    \n",
        "def videoset_csv_reader(csv_file, dictionary, istrain):\n",
        "    # read omg_TrainVideos.csv and omg_ValidationVideos.csv\n",
        "    counter = 0\n",
        "    with open(csv_file, 'r') as fin:\n",
        "        reader = csv.DictReader(fin)\n",
        "        for row in reader:\n",
        "            video = row['video']\n",
        "            utterance = row['utterance']\n",
        "            utter_index = utterance.split('.')[0].split('_')[-1]\n",
        "            if istrain:\n",
        "                m_labels = [float(row['arousal']), float(row['valence']), int(row['EmotionMaxVote'])]\n",
        "            else:\n",
        "                m_labels = []\n",
        "            if video not in dictionary.keys():\n",
        "                dictionary[video] = {}\n",
        "            dictionary[video][utter_index] = m_labels\n",
        "            counter +=1\n",
        "    return counter\n",
        "\n",
        "def load_pickle(file_name):\n",
        "    with open(file_name, 'rb') as fin:\n",
        "        return pickle.load(fin)\n",
        "    \n",
        "def print_out_csv(arousal_pred, valence_pred, name_list, refer_csv, out_csv):\n",
        "    data = {}\n",
        "    #laod prediction\n",
        "    for index, ids in enumerate(name_list):\n",
        "        vid,uttr = ids\n",
        "        prediction = [arousal_pred[index], valence_pred[index]]\n",
        "        if vid not in data.keys():\n",
        "            data[vid] = {}\n",
        "        data[vid][uttr] = prediction\n",
        "    # print out in order\n",
        "    df = pandas.read_csv(refer_csv)\n",
        "    videos = df['video']\n",
        "    utterances = df['utterance']\n",
        "    new_df = pandas.DataFrame({'video':[], 'utterance':[], 'arousal':[],'valence':[]})\n",
        "    new_df['video'] = videos\n",
        "    new_df['utterance'] = utterances\n",
        "    arousal = []\n",
        "    valence = []\n",
        "    for vid,utter in zip(videos, utterances):\n",
        "        uttr_index = utter.split('.')[0].split('_')[-1]\n",
        "        try:\n",
        "            a = data[vid][uttr_index][0]\n",
        "        except:\n",
        "            print(\"{} arousal prediction is missing!\".format(vid+':'+utter))\n",
        "            a = 0.0\n",
        "        try:\n",
        "            v = data[vid][uttr_index][1]\n",
        "        except:\n",
        "            print(\"{} arousal prediction is missing!\".format(vid+':'+utter))\n",
        "            v = 0.0\n",
        "        arousal.append(a)\n",
        "        valence.append(v)\n",
        "    new_df['arousal'] = arousal\n",
        "    new_df['valence'] = valence\n",
        "    new_df[['video', 'utterance','arousal','valence']].to_csv(out_csv, index=False)\n",
        "    print(\"csv file printed out successfully!\")\n",
        "            \n",
        "def get_capitalized_name(name):\n",
        "    return name.replace('_', ' ').title().replace(' ', '_')\n",
        "\n",
        "class DataSet():\n",
        "\n",
        "    def __init__(self, istrain=True, model='audio_feature', task='category', seq_length=20, model_name='audio_feature', is_fusion=False, also_test=True):  # initialization, the length of sequences in one video is 20 (modifiable)\n",
        "        self.istrain = istrain\n",
        "        self.model = model\n",
        "        self.model_type = model\n",
        "        self.task = task\n",
        "        self.seq_length = seq_length\n",
        "        self.model_name = model_name\n",
        "        self.is_fusion = is_fusion\n",
        "        if is_fusion:\n",
        "            self.model_type = 'multimodal'\n",
        "        self.features = []\n",
        "        self.feature_data = {}\n",
        "        self.list_features = all_features.keys()\n",
        "        \n",
        "        self.train_video_csv_path = os.path.join(data_folder, 'train.csv') #OK\n",
        "        self.validation_video_csv_path = os.path.join(data_folder, 'validation.csv') #OK\n",
        "\n",
        "        self.test_video_csv_path = os.path.join(data_folder, 'test.csv') #OK\n",
        "\n",
        "        #self.test_video_path = os.path.join('..','new_omg_ValidationVideos.csv')\n",
        "        # Get the data, including video, utterance and labels\n",
        "        self.data = self.get_data(also_test)\n",
        "        self.paths = {}\n",
        "        for feature_name in self.list_features:\n",
        "          cap_name = get_capitalized_name(feature_name)\n",
        "          self.paths[feature_name] = [os.path.join(data_folder, cap_name + '_Train.pkl'),\n",
        "                                      os.path.join(data_folder, cap_name + '_Validation.pkl'),\n",
        "                                      os.path.join(data_folder, cap_name + '_Test.pkl')]\n",
        "\n",
        "        if not is_fusion:\n",
        "            self.features = [self.model]\n",
        "        else:\n",
        "            self.features = self.model.split(',')\n",
        "\n",
        "        self.load_neccessary(self.model, is_fusion=is_fusion)\n",
        "\n",
        " \n",
        "    def get_data(self, also_test=False):\n",
        "        data = {}\n",
        "\n",
        "        # train data stored in data['train']\n",
        "        data['Train'] = {}\n",
        "        train_counter = videoset_csv_reader(self.train_video_csv_path, data['Train'], self.istrain)\n",
        "    \n",
        "        # validation data stored in data['validation']\n",
        "        data['Validation'] = {}\n",
        "        valid_counter = videoset_csv_reader(self.validation_video_csv_path, data['Validation'], self.istrain)\n",
        "        print(\"The dataset has been split to: train set:{} videos, {} utterances; validation set: {} videos, {} utterances. \".format(\n",
        "            len(data['Train'].keys()), train_counter, \n",
        "            len(data['Validation'].keys()), valid_counter))\n",
        "        if not self.istrain or also_test:\n",
        "        # test data stored in data['validation']\n",
        "            data['Test'] = {}\n",
        "            test_counter = videoset_csv_reader(self.test_video_csv_path, data['Test'], self.istrain)\n",
        "            print(\"Evaluation for test set: {} videos, {} utterances. \".format(\n",
        "                   len(data['Test'].keys()), test_counter))\n",
        "\n",
        "        return data\n",
        "\n",
        "    def unroll_and_normalize(self, feature_dict):\n",
        "        # So the normalization will be done in train set, validation set and (test set)\n",
        "        normalized_f_dict = {}\n",
        "        scaler = MinMaxScaler()\n",
        "        if self.istrain:\n",
        "            states = ['Train', 'Validation','Test']\n",
        "        else:\n",
        "            states = ['Train', 'Validation','Test']\n",
        "        for state in states:\n",
        "            main_dict = feature_dict[state]\n",
        "            videos = main_dict.keys()\n",
        "            features = []\n",
        "            indexes = []\n",
        "            for vid in videos:\n",
        "                utterances = main_dict[vid].keys()\n",
        "                for uttr in utterances:\n",
        "                    indexes.append([vid, uttr])\n",
        "                    feature = np.asarray(main_dict[vid][uttr])\n",
        "                    if len(feature.shape) == 2:\n",
        "                        #(time_steps, feature_dim)\n",
        "                        time_steps = feature.shape[0]\n",
        "                        feature_dim = feature.shape[1]\n",
        "                        is_time = True\n",
        "                    elif len(feature.shape) == 1:\n",
        "                        feature_dim = feature.shape[0]\n",
        "                        is_time = False\n",
        "                    features.append(feature)\n",
        "            #reshape if time dimension exists, and normalize by scale\n",
        "            if is_time:\n",
        "                unrolled_f = np.asarray(features).reshape((-1,feature_dim))\n",
        "                if state == 'Train':\n",
        "                    scaled_f = scaler.fit_transform(unrolled_f)\n",
        "                    scaled_f = scaled_f.reshape((-1, time_steps, feature_dim))\n",
        "                else:\n",
        "                    scaled_f = scaler.transform(unrolled_f)\n",
        "                    scaled_f = scaled_f.reshape((-1, time_steps, feature_dim))\n",
        "            else:\n",
        "                unrolled_f  = np.asarray(features)\n",
        "                if state == 'Train':\n",
        "                    scaled_f = scaler.fit_transform(unrolled_f)\n",
        "                else:\n",
        "                    scaled_f = scaler.transform(unrolled_f)\n",
        "            for i in range(scaled_f.shape[0]):\n",
        "                vid,uttr = indexes[i]\n",
        "                main_dict[vid][uttr] = scaled_f[i]\n",
        "            normalized_f_dict[state] = main_dict\n",
        "        return normalized_f_dict\n",
        "                    \n",
        "    def load_neccessary(self, model_type, is_fusion=False):\n",
        "        if not is_fusion and ',' not in model_type:\n",
        "            features = [model_type]\n",
        "        else:\n",
        "            features = model_type.split(',')\n",
        "        for model in features:\n",
        "            if '[' in model:\n",
        "                model = model.replace(':',',')[1:-1]\n",
        "                self.load_neccessary(model, is_fusion)\n",
        "            else:\n",
        "                self.load_feature_dynamic(model)\n",
        "                #func = getattr(self, 'load_' + model)\n",
        "                #func()\n",
        "\n",
        "    def load_feature(self, path_list):\n",
        "        #print('path_list',path_list)\n",
        "        if len(path_list) == 1:\n",
        "            #only contains train and validation\n",
        "            with open(path_list[0],'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        elif len(path_list) == 2:\n",
        "            feature = {}\n",
        "            with open(path_list[0],'rb') as f:\n",
        "                try:\n",
        "                    feature =  pickle.load(f, encoding='latin1')\n",
        "                except:\n",
        "                    feature =  pickle.load(f)\n",
        "            with open(path_list[1],'rb') as f:\n",
        "                try:\n",
        "                    data = pickle.load(f, encoding='latin1')\n",
        "                except:\n",
        "                    data = pickle.load(f)\n",
        "                if len(data.keys()) == 1:\n",
        "                    if list(data.keys())[0] == 'Test':\n",
        "                        feature['Test'] = data['Test']\n",
        "                    else:\n",
        "                        feature['Test'] = data\n",
        "                else:\n",
        "                    feature['Test'] = data\n",
        "        elif len(path_list) == 3:\n",
        "            states = ['Train', 'Validation', 'Test']\n",
        "            feature = {}\n",
        "            for i,state in enumerate(states):\n",
        "              with open(path_list[i],'rb') as f:\n",
        "                  try:\n",
        "                      data = pickle.load(f, encoding='latin1')\n",
        "                  except:\n",
        "                      data = pickle.load(f)\n",
        "                  if len(data.keys()) == 1:\n",
        "                      if list(data.keys())[0] == state:\n",
        "                          feature[state] = data[state]\n",
        "                      else:\n",
        "                          feature[state] = data\n",
        "                  else:\n",
        "                      feature[state] = data\n",
        "            return feature\n",
        "        \n",
        "    def load_feature_dynamic(self, feature_name):\n",
        "        if 'fusion' in feature_name:\n",
        "          fused_data = self.load_feature_fusion(feature_name)\n",
        "          self.feature_data[feature_name] = self.unroll_and_normalize(fused_data)\n",
        "        else:\n",
        "          self.feature_data[feature_name] = self.unroll_and_normalize(self.load_feature(self.paths[feature_name]))\n",
        "\n",
        "    def load_feature_fusion(self, feature_name):\n",
        "        output = None\n",
        "        features = []\n",
        "        feature_base = feature_name.split('_')[0]\n",
        "        for feature in self.list_features:\n",
        "          if feature_base in feature and 'fusion' not in feature:\n",
        "              features.append(feature)\n",
        "        print(features)\n",
        "        if len(features) == 1:\n",
        "          return self.load_feature_dynamic(features[0])\n",
        "        elif len(features) > 1:\n",
        "          fused = {}\n",
        "          base_feature = features.pop(0)\n",
        "          base_data = self.load_feature(self.paths[base_feature])\n",
        "          feature_data = {}\n",
        "          for feature in features:\n",
        "            feature_data[feature] = self.load_feature(self.paths[feature])\n",
        "          states = base_data.keys()\n",
        "          for state in states:\n",
        "            fused[state] = {}\n",
        "            videos = base_data[state].keys()\n",
        "            for vid in videos:\n",
        "              found = True\n",
        "              for feature in features:\n",
        "                if not vid in feature_data[feature][state].keys():\n",
        "                  found = False\n",
        "              if found:\n",
        "                fused[state][vid] = {}\n",
        "                utters = base_data[state][vid].keys()\n",
        "                for uttr in utters:\n",
        "                  found = True\n",
        "                  for feature in features:\n",
        "                    if not uttr in feature_data[feature][state][vid].keys():\n",
        "                      found = False\n",
        "                  if found:\n",
        "                    base_uttr = base_data[state][vid][uttr]\n",
        "                    fused_array = base_uttr\n",
        "                    for feature in features:\n",
        "                      cur_uttr = feature_data[feature][state][vid][uttr]\n",
        "                      if type(base_uttr) == list:\n",
        "                        base_uttr = np.array(base_uttr)\n",
        "                      if type(cur_uttr) == list:\n",
        "                        cur_uttr = np.array(cur_uttr)\n",
        "                      if len(base_uttr.shape) == 2:\n",
        "                        assert base_uttr.shape[0] == cur_uttr.shape[0]\n",
        "                      fused_array = np.concatenate((fused_array, cur_uttr), axis = -1)\n",
        "                    fused[state][vid][uttr] = fused_array\n",
        "          return fused\n",
        "        return output\n",
        "        \n",
        "    def load_face_fusion(self):\n",
        "        visual_f_part0 = self.load_feature(self.face_feature_path)\n",
        "        visual_f_part1 = self.load_feature(self.face_visual_path)\n",
        "        #fuse two parts\n",
        "        fused_visual_f = {}\n",
        "        states = visual_f_part0.keys()\n",
        "        for state in states:\n",
        "            fused_visual_f[state] = {}\n",
        "            videos = visual_f_part0[state].keys()\n",
        "            for vid in videos:\n",
        "                if vid in visual_f_part1[state].keys():\n",
        "                    fused_visual_f[state][vid] = {}\n",
        "                    utters = visual_f_part0[state][vid].keys()\n",
        "                    for uttr in utters:\n",
        "                        if uttr in visual_f_part1[state][vid].keys():\n",
        "                            f0 = visual_f_part0[state][vid][uttr]\n",
        "                            f1 = visual_f_part1[state][vid][uttr]\n",
        "                            assert f0.shape[0] == f1.shape[0]\n",
        "                            fused_visual_f[state][vid][uttr] = np.concatenate((f0,f1), axis = -1)\n",
        "        self.face_fusion = self.unroll_and_normalize(fused_visual_f)\n",
        "\n",
        "    def load_body_fusion(self):\n",
        "        body_visual_f_part0 = self.load_feature(self.body_feature_path)\n",
        "        body_visual_f_part1 = self.load_feature(self.body_visual_path)\n",
        "        #fuse two parts\n",
        "        fused_body_fusion_f = {}\n",
        "        states = body_visual_f_part0.keys()\n",
        "        for state in states:\n",
        "            fused_body_fusion_f[state] = {}\n",
        "            videos = body_visual_f_part0[state].keys()\n",
        "            for vid in videos:\n",
        "                if vid in body_visual_f_part1[state].keys():\n",
        "                    fused_body_fusion_f[state][vid] = {}\n",
        "                    utters = body_visual_f_part0[state][vid].keys()\n",
        "                    for uttr in utters:\n",
        "                        if uttr in body_visual_f_part1[state][vid].keys():\n",
        "                            f0 = np.asarray(body_visual_f_part0[state][vid][uttr])\n",
        "                            f1 = body_visual_f_part1[state][vid][uttr]\n",
        "                            assert f0.shape[0] == f1.shape[0]\n",
        "                            fused_body_fusion_f[state][vid][uttr] = np.concatenate((f0,f1), axis=-1)\n",
        "        self.body_fusion = self.unroll_and_normalize(fused_body_fusion_f)\n",
        "\n",
        "    def load_word_fusion(self):\n",
        "        word_f_part0 = self.load_feature(self.word_feature_path)\n",
        "        word_f_part1 = self.load_feature(self.word_mpqa_path)\n",
        "        #fuse two parts\n",
        "        fused_word_f = {}\n",
        "        states = word_f_part0.keys()\n",
        "        for state in states:\n",
        "            fused_word_f[state] = {}\n",
        "            videos = word_f_part0[state].keys()\n",
        "            for vid in videos:\n",
        "                if vid in word_f_part1[state].keys():\n",
        "                    fused_word_f[state][vid] = {}\n",
        "                    utters = word_f_part0[state][vid].keys()\n",
        "                    for uttr in utters:\n",
        "                        if uttr in word_f_part1[state][vid].keys():\n",
        "                            f0 = np.asarray(word_f_part0[state][vid][uttr])\n",
        "                            f1 = np.asarray(word_f_part1[state][vid][uttr])\n",
        "        \n",
        "                            fused_word_f[state][vid][uttr] = np.concatenate((f0,f1), axis = -1)\n",
        "        self.word_fusion = self.unroll_and_normalize(fused_word_f)\n",
        "\n",
        "    def process_sequence(self, list):\n",
        "        # make the sequence length is self.seq_length\n",
        "        length = len(list)\n",
        "        assert length > 0\n",
        "        if length >= self.seq_length:\n",
        "            return list[:self.seq_length]\n",
        "        else:\n",
        "            for _ in range(self.seq_length-length):\n",
        "                list.append(np.zeros(list[0].shape))\n",
        "            return list\n",
        "\n",
        "    def get_dynamic_feature(self,vid,uttr, mode, feature_name):\n",
        "        state_name = mode\n",
        "        try:\n",
        "            utter_feature = self.feature_data[feature_name][state_name][vid][uttr]\n",
        "        except:\n",
        "            print(\"Error when access to \"+vid+' '+'utterance_'+uttr+' in '+feature_name)\n",
        "            return None\n",
        "        else:\n",
        "            return utter_feature\n",
        "\n",
        "    def get_values(self, vid, uttr, mode, features):\n",
        "        state_name = mode\n",
        "        utter_feature = []\n",
        "        try:\n",
        "            for feature in features:\n",
        "                #attr = getattr(self, feature)\n",
        "                #utter_feature.append(attr[state_name][vid][uttr])\n",
        "                utter_feature.append(self.feature_data[feature][state_name][vid][uttr])\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(\"Error when access to \"+vid+' '+'utterance_'+uttr+' in modal!')\n",
        "            utter_feature = None\n",
        "        return utter_feature\n",
        "\n",
        "    def get_label(self, train_valid_test, vid, uttr):\n",
        "        #according to task, return arousal, valence or emotion category\n",
        "        if self.task == 'arousal':\n",
        "            return self.data[train_valid_test][vid][uttr][0]\n",
        "        elif self.task == 'valence':\n",
        "            return self.data[train_valid_test][vid][uttr][1]\n",
        "        else:\n",
        "            return self.data[train_valid_test][vid][uttr][2]\n",
        "\n",
        "    def get_feature_size(self, feature):\n",
        "        size_dict = {'bimodal':2, 'trimodal':3, 'quadmodal':4, '[':-1}\n",
        "        size = 1\n",
        "        for nom_feature in size_dict:\n",
        "            if nom_feature in feature:\n",
        "                size = size_dict[nom_feature]\n",
        "                if size == -1:\n",
        "                    size = len(feature.split(':'))\n",
        "        return size\n",
        "\n",
        "    def calc_num_features(self):\n",
        "        total = 0\n",
        "        for feature in self.features:\n",
        "            total += self.get_feature_size(feature)\n",
        "        return total\n",
        "\n",
        "    def get_all_sequences_in_memory(self, train_valid_test):\n",
        "        \"\"\"\n",
        "        :param train_valid_test:\n",
        "        :param task_type: 'arousal','valence' or 'category'\n",
        "        :param feature_type: 'visual','arousal','word'\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        \n",
        "        data = self.data\n",
        "        \n",
        "        if train_valid_test in ['Train','Validation','Test']:\n",
        "            main_dict = data[train_valid_test]\n",
        "            print(\"Loading %s dataset with %d videos.\"%(train_valid_test, len(main_dict.keys())))\n",
        "            x_feature = []\n",
        "            x_audio = []\n",
        "            x_visual = []\n",
        "            x_word = []\n",
        "            x_body = []\n",
        "            name_list = []\n",
        "            y = []\n",
        "\n",
        "            videos = main_dict.keys()\n",
        "\n",
        "            if self.model_type == 'multimodal':\n",
        "                num_features = self.calc_num_features()\n",
        "                values = [[] for i in range(num_features)]\n",
        "                for vid in videos:\n",
        "                    utterances = sorted(main_dict[vid].keys(), key = int)\n",
        "                    for uttr in utterances:\n",
        "                        has_value = True\n",
        "                        row = []\n",
        "                        for feature in self.features:\n",
        "                            if '[' in feature:\n",
        "                                features = feature.replace(':',',')[1:-1].split(',')\n",
        "                                value = self.get_values(vid, uttr, train_valid_test, features)\n",
        "                            else:\n",
        "                                #func = getattr(self, 'get_' + feature)\n",
        "                                #value = func(vid, uttr, train_valid_test)\n",
        "                                value = self.get_dynamic_feature(vid, uttr, train_valid_test, feature)\n",
        "                            if value is None:\n",
        "                                has_value = False\n",
        "                                break\n",
        "                            else:\n",
        "                                feature_size = self.get_feature_size(feature)\n",
        "                                if feature_size == 1:\n",
        "                                    row.append(value)\n",
        "                                else:\n",
        "                                    for val in value:\n",
        "                                        row.append(val)\n",
        "                        if has_value:\n",
        "                            for i in range(num_features):\n",
        "                                values[i].append(row[i])\n",
        "                            if self.istrain:\n",
        "                                y.append(self.get_label(train_valid_test, vid, uttr))\n",
        "                            name_list.append([vid, uttr])\n",
        "                for i in range(num_features):\n",
        "                    values[i] = np.asarray(values[i])\n",
        "                x = values\n",
        "                if self.istrain:\n",
        "                    y = np.asarray(y)                \n",
        "            else:\n",
        "                for vid in videos:\n",
        "                    utterances = sorted(main_dict[vid].keys(), key = int)\n",
        "                    for uttr in utterances:\n",
        "                        b = self.get_dynamic_feature(vid, uttr, train_valid_test, self.model_type)\n",
        "                        if (b is not None) :\n",
        "                            x_feature.append(b)\n",
        "                            if self.istrain:\n",
        "                                y.append(self.get_label(train_valid_test, vid, uttr))\n",
        "                            name_list.append([vid, uttr])\n",
        "                x =  np.asarray(x_feature)\n",
        "                if self.istrain:\n",
        "                    y = np.asarray(y)\n",
        "\n",
        "            if self.istrain:\n",
        "                return x, y, name_list\n",
        "            else:\n",
        "                return x, name_list\n",
        "              \n",
        "a, b=4.0, 8.0\n",
        "class ResearchModels():\n",
        "    def __init__(self, istrain= True, model='audio_feature', seq_length = 20,\n",
        "                 saved_path=None, task_type = 'category', \n",
        "                  learning_r = 1e-3, model_name='audio_feature', \n",
        "                  is_fusion=False, fusion_type='early'):\n",
        "        # set defaults\n",
        "        self.istrain = istrain\n",
        "        self.model = model\n",
        "        self.seq_length = seq_length\n",
        "        self.saved_path = saved_path\n",
        "        self.task_type = task_type\n",
        "        self.model_name = model_name\n",
        "        self.is_fusion = is_fusion\n",
        "        self.fusion_type = fusion_type\n",
        "        # Get the appropriate model.\n",
        "        if not self.is_fusion and '[' not in model:\n",
        "            if (self.saved_path is not None):\n",
        "                print(\"Loading model %s\" % self.saved_path.split('/')[-1])\n",
        "                self.model = self.load_custom(self.saved_path)\n",
        "            elif model in all_features.keys():\n",
        "                if all_features[model]['temporal']:\n",
        "                  self.input_shape = (seq_length, all_features[model]['size'])\n",
        "                else:\n",
        "                  self.input_shape = (all_features[model]['size'],)\n",
        "                model_name = self.get_model_name(model)\n",
        "                func = getattr(self, model_name)\n",
        "                self.model = func(model)\n",
        "            else:\n",
        "              print(\"Unknown network.\")\n",
        "              sys.exit()\n",
        "        else:\n",
        "            print(\"Loading fusion model.\")\n",
        "            self.model = self.get_fusion(models=model.split(','), fusion_type=fusion_type)\n",
        "\n",
        "        # Now compile the network.\n",
        "        print (self.model.summary())\n",
        "        sgd = SGD(lr = learning_r, decay = 1e-3, momentum = 0.9, nesterov = True)\n",
        "        adam = Adam(lr = learning_r, decay = 1e-3, beta_1=0.9, beta_2=0.999)\n",
        "        if task_type == 'category':\n",
        "            self.model.compile(loss = 'mean_squared_error', metrics = ['accuracy'], optimizer = adam)\n",
        "        else:\n",
        "            self.model.compile(loss = ccc_loss, metrics = ['accuracy',mse,ccc_metric], optimizer = sgd)\n",
        "            #self.model.compile(loss = 'mean_squared_error', metrics = ['accuracy',mse,ccc_metric], optimizer = sgd)\n",
        "\n",
        "    def get_model_name(self, model):\n",
        "      model_name = ''\n",
        "      if model in all_features.keys():\n",
        "        if all_features[model]['temporal']:\n",
        "          model_name = 'generic_rnn_model'\n",
        "        else:\n",
        "          model_name = 'generic_dense_model'\n",
        "        if 'model' in all_features[model]:\n",
        "          model_name = all_features[model]['model']\n",
        "      return model_name\n",
        "      \n",
        "    def get_fusion(self, models, fusion_type):\n",
        "        rand = self.get_random()\n",
        "        model_dict = {}\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "        fusion = None\n",
        "        for model in models:\n",
        "            if '[' in model:\n",
        "                model_values = model.replace(':',',')[1:-1].split(',')\n",
        "                self.dynamic_model = model_values\n",
        "                model = 'multimodal' + rand\n",
        "                model_dict[model] = self.get_fusion(model_values, fusion_type)\n",
        "            else:\n",
        "                model_name = self.get_model_name(model)\n",
        "                func = getattr(self, model_name)\n",
        "                model_dict[model] = func(model)\n",
        "            #if fusion_type == 'early':\n",
        "            model_dict[model].layers.pop()\n",
        "            if type(model_dict[model].input) == list:\n",
        "                #inputs.append(concatenate(model_dict[model].input))\n",
        "                inputs += model_dict[model].input\n",
        "            else:\n",
        "                inputs.append(model_dict[model].input)\n",
        "            outputs.append(model_dict[model].layers[-1].output)\n",
        "        x = concatenate(outputs)\n",
        "        if fusion_type == 'early':\n",
        "            x = Dense(1024)(x)\n",
        "            x = Activation('relu')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "        out = self.decision_layer('multimodal' + rand)(x)\n",
        "        print(inputs, len(inputs))\n",
        "        fusion = Model(inputs, out)\n",
        "        return fusion\n",
        "        \n",
        "    def load_custom(self,pretrained_path):\n",
        "        model = load_model(pretrained_path, \n",
        "                           custom_objects ={'ccc_metric':ccc_metric,\n",
        "                                            'ccc_loss': ccc_loss})\n",
        "        return model\n",
        "        \n",
        "    def decision_layer(self,name):\n",
        "        if self.task_type == 'arousal':\n",
        "            # add the output layer\n",
        "            dl=Dense(1, activation ='sigmoid' , kernel_initializer ='normal', name = name+'_decision_layer')\n",
        "        elif self.task_type == 'valence':\n",
        "            dl = Dense(1, activation= 'tanh' ,kernel_initializer ='normal',name = name+'_decision_layer' )\n",
        "        elif self.task_type == 'category':\n",
        "            dl = Dense(num_classes, activation= 'softmax' ,kernel_initializer ='normal',name = name+'_decision_layer')\n",
        "        return dl\n",
        "    \n",
        "    def add_hidden_layer(self, model, name):\n",
        "        # the hidden layer block\n",
        "        model.add(Dense(256, name = name+'_hidden_layer'))\n",
        "        model.add(Activation('relu', name = name+'_activation'))\n",
        "        model.add(BatchNormalization(name = name+'_BN'))\n",
        "        model.add(Dropout(0.5, name = name+'_dropout'))\n",
        "        return model\n",
        "\n",
        "    def get_random(self):\n",
        "        return '_'+str(randint(0, 100))\n",
        "\n",
        "    def generic_rnn_model(self, feature):\n",
        "        # when input is visual feature\n",
        "        rand = self.get_random()\n",
        "        model = Sequential()\n",
        "        model.add(BatchNormalization(input_shape = (self.seq_length, all_features[feature]['size']), name = feature+'_BN_1'+rand))\n",
        "        model.add(AveragePooling1D(pool_size = 2 , name = feature+'_average'+rand))\n",
        "        \n",
        "        \n",
        "        # lstm layer\n",
        "        model.add(LSTM(lstm_size, name  = feature+'_lstm'+rand))\n",
        "        model.add(Activation('relu', name = feature+'_activation1'+rand))\n",
        "        model.add(BatchNormalization(name = feature+'_BN_2'+rand))\n",
        "        model.add(Dropout(0.5, name = feature+'_dropout_1'+rand))\n",
        "        \n",
        "        # the hidden layer\n",
        "        model = self.add_hidden_layer(model, feature+'_hidden'+rand)\n",
        "        \n",
        "        # the decision layer\n",
        "        model.add(self.decision_layer(feature+rand))\n",
        "        \n",
        "        return model\n",
        "\n",
        "    def generic_dense_model(self, feature):\n",
        "        rand = self.get_random()\n",
        "        model = Sequential()\n",
        "        # the input layer\n",
        "        model.add(BatchNormalization(input_shape = (all_features[feature]['size'],), name = feature+'_BN_1'+rand))\n",
        "   \n",
        "        # add the hidden layer\n",
        "        model = self.add_hidden_layer(model,feature+'_hidden'+rand)\n",
        "\n",
        "        #add the decision layer\n",
        "        model.add(self.decision_layer(feature+rand))\n",
        "        return model\n",
        "      \n",
        "def _moments(x, axes, shift=None, keep_dims=False):\n",
        "    ''' Wrapper over tensorflow backend call '''\n",
        "    if K.backend() == 'tensorflow':\n",
        "        import tensorflow as tf\n",
        "        return tf.nn.moments(x, axes, shift=shift, keep_dims=keep_dims)\n",
        "    elif K.backend() == 'theano':\n",
        "        import theano.tensor as T\n",
        "\n",
        "        mean_batch = T.mean(x, axis=axes, keepdims=keep_dims)\n",
        "        var_batch = T.var(x, axis=axes, keepdims=keep_dims)\n",
        "        return mean_batch, var_batch\n",
        "    else:\n",
        "        raise RuntimeError(\"Currently does not support CNTK backend\")\n",
        "\n",
        "\n",
        "class BatchRenormalization(Layer):\n",
        "    def __init__(self, axis=-1, momentum=0.99, center=True,\n",
        "                 scale=True, epsilon=1e-3,\n",
        "                 r_max_value=3., d_max_value=5.,\n",
        "                 t_delta=1e-3, weights=None, beta_initializer='zero',\n",
        "                 gamma_initializer='one', moving_mean_initializer='zeros',\n",
        "                 moving_variance_initializer='ones',\n",
        "                 gamma_regularizer=None, beta_regularizer=None,\n",
        "                 beta_constraint=None, gamma_constraint=None, **kwargs):\n",
        "        if axis != -1 and K.backend() == 'tensorflow':\n",
        "            raise NotImplementedError('There is currently a bug '\n",
        "                                      'when using batch renormalisation and '\n",
        "                                      'the TensorFlow backend.')\n",
        "\n",
        "        warnings.warn('This implementation of BatchRenormalization is inconsistent with the '\n",
        "                      'original paper and therefore results may not be similar ! '\n",
        "                      'For discussion on the inconsistency of this implementation, '\n",
        "                      'refer here : https://github.com/keras-team/keras-contrib/issues/17')\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "        self.center = center\n",
        "        self.scale = scale\n",
        "        self.momentum = momentum\n",
        "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
        "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
        "        self.initial_weights = weights\n",
        "        self.r_max_value = r_max_value\n",
        "        self.d_max_value = d_max_value\n",
        "        self.t_delta = t_delta\n",
        "        self.beta_initializer = initializers.get(beta_initializer)\n",
        "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
        "        self.moving_mean_initializer = initializers.get(moving_mean_initializer)\n",
        "        self.moving_variance_initializer = initializers.get(\n",
        "            moving_variance_initializer)\n",
        "        self.beta_constraint = constraints.get(beta_constraint)\n",
        "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
        "\n",
        "        super(BatchRenormalization, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[self.axis]\n",
        "        if dim is None:\n",
        "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
        "                                                        'input tensor should have a defined dimension '\n",
        "                                                        'but the layer received an input with shape ' +\n",
        "                             str(input_shape) + '.')\n",
        "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
        "                                    axes={self.axis: dim})\n",
        "        shape = (dim,)\n",
        "\n",
        "        if self.scale:\n",
        "            self.gamma = self.add_weight(shape=shape,\n",
        "                                         initializer=self.gamma_initializer,\n",
        "                                         regularizer=self.gamma_regularizer,\n",
        "                                         constraint=self.gamma_constraint,\n",
        "                                         name='{}_gamma'.format(self.name))\n",
        "        else:\n",
        "            self.gamma = None\n",
        "\n",
        "        if self.center:\n",
        "            self.beta = self.add_weight(shape=shape,\n",
        "                                        initializer=self.beta_initializer,\n",
        "                                        regularizer=self.beta_regularizer,\n",
        "                                        constraint=self.beta_constraint,\n",
        "                                        name='{}_beta'.format(self.name))\n",
        "        else:\n",
        "            self.beta = None\n",
        "\n",
        "        self.running_mean = self.add_weight(shape=shape,\n",
        "                                            initializer=self.moving_mean_initializer,\n",
        "                                            name='{}_running_mean'.format(self.name),\n",
        "                                            trainable=False)\n",
        "\n",
        "        self.running_variance = self.add_weight(\n",
        "            shape=shape,\n",
        "            initializer=self.moving_variance_initializer,\n",
        "            name='{}_running_std'.format(self.name),\n",
        "            trainable=False)\n",
        "\n",
        "        self.r_max = K.variable(1, name='{}_r_max'.format(self.name))\n",
        "\n",
        "        self.d_max = K.variable(0, name='{}_d_max'.format(self.name))\n",
        "\n",
        "        self.t = K.variable(0, name='{}_t'.format(self.name))\n",
        "\n",
        "        self.t_delta_tensor = K.constant(self.t_delta)\n",
        "\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        assert self.built, 'Layer must be built before being called'\n",
        "        input_shape = K.int_shape(inputs)\n",
        "\n",
        "        reduction_axes = list(range(len(input_shape)))\n",
        "        del reduction_axes[self.axis]\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
        "\n",
        "        mean_batch, var_batch = _moments(inputs, reduction_axes,\n",
        "                                         shift=None, keep_dims=False)\n",
        "        std_batch = (K.sqrt(var_batch + self.epsilon))\n",
        "\n",
        "        r = std_batch / (K.sqrt(self.running_variance + self.epsilon))\n",
        "        r = K.stop_gradient(K.clip(r, 1 / self.r_max, self.r_max))\n",
        "\n",
        "        d = (mean_batch - self.running_mean) / K.sqrt(self.running_variance\n",
        "                                                      + self.epsilon)\n",
        "        d = K.stop_gradient(K.clip(d, -self.d_max, self.d_max))\n",
        "\n",
        "        if sorted(reduction_axes) == range(K.ndim(inputs))[:-1]:\n",
        "            x_normed_batch = (inputs - mean_batch) / std_batch\n",
        "            x_normed = (x_normed_batch * r + d) * self.gamma + self.beta\n",
        "        else:\n",
        "            # need broadcasting\n",
        "            broadcast_mean = K.reshape(mean_batch, broadcast_shape)\n",
        "            broadcast_std = K.reshape(std_batch, broadcast_shape)\n",
        "            broadcast_r = K.reshape(r, broadcast_shape)\n",
        "            broadcast_d = K.reshape(d, broadcast_shape)\n",
        "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
        "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
        "\n",
        "            x_normed_batch = (inputs - broadcast_mean) / broadcast_std\n",
        "            x_normed = (x_normed_batch * broadcast_r\n",
        "                        + broadcast_d) * broadcast_gamma + broadcast_beta\n",
        "\n",
        "        # explicit update to moving mean and standard deviation\n",
        "        mean_update = K.moving_average_update(self.running_mean,\n",
        "                                              mean_batch,\n",
        "                                              self.momentum)\n",
        "        variance_update = K.moving_average_update(self.running_variance,\n",
        "                                                  std_batch ** 2,\n",
        "                                                  self.momentum)\n",
        "        self.add_update([mean_update, variance_update], inputs)\n",
        "\n",
        "        # update r_max and d_max\n",
        "        r_val = self.r_max_value / (1 + (self.r_max_value - 1) * K.exp(-self.t))\n",
        "        d_val = (self.d_max_value\n",
        "                 / (1 + ((self.d_max_value / 1e-3) - 1) * K.exp(-(2 * self.t))))\n",
        "\n",
        "        self.add_update([K.update(self.r_max, r_val),\n",
        "                         K.update(self.d_max, d_val),\n",
        "                         K.update_add(self.t, self.t_delta_tensor)], inputs)\n",
        "\n",
        "        if training in {0, False}:\n",
        "            return x_normed\n",
        "        else:\n",
        "            def normalize_inference():\n",
        "                if sorted(reduction_axes) == list(range(K.ndim(inputs)))[:-1]:\n",
        "                    x_normed_running = K.batch_normalization(\n",
        "                        inputs, self.running_mean, self.running_variance,\n",
        "                        self.beta, self.gamma,\n",
        "                        epsilon=self.epsilon)\n",
        "\n",
        "                    return x_normed_running\n",
        "                else:\n",
        "                    # need broadcasting\n",
        "                    broadcast_running_mean = K.reshape(self.running_mean,\n",
        "                                                       broadcast_shape)\n",
        "                    broadcast_running_std = K.reshape(self.running_variance,\n",
        "                                                      broadcast_shape)\n",
        "                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
        "                    broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
        "                    x_normed_running = K.batch_normalization(\n",
        "                        inputs, broadcast_running_mean, broadcast_running_std,\n",
        "                        broadcast_beta, broadcast_gamma,\n",
        "                        epsilon=self.epsilon)\n",
        "\n",
        "                    return x_normed_running\n",
        "\n",
        "            # pick the normalized form of inputs corresponding to the training phase\n",
        "            # for batch renormalization, inference time remains same as batchnorm\n",
        "            x_normed = K.in_train_phase(x_normed, normalize_inference,\n",
        "                                        training=training)\n",
        "\n",
        "            return x_normed\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'epsilon': self.epsilon,\n",
        "            'axis': self.axis,\n",
        "            'center': self.center,\n",
        "            'scale': self.scale,\n",
        "            'momentum': self.momentum,\n",
        "            'gamma_regularizer': initializers.serialize(self.gamma_regularizer),\n",
        "            'beta_regularizer': initializers.serialize(self.beta_regularizer),\n",
        "            'moving_mean_initializer': initializers.serialize(\n",
        "                self.moving_mean_initializer),\n",
        "            'moving_variance_initializer': initializers.serialize(\n",
        "                self.moving_variance_initializer),\n",
        "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
        "            'gamma_constraint': constraints.serialize(self.gamma_constraint),\n",
        "            'r_max_value': self.r_max_value,\n",
        "            'd_max_value': self.d_max_value,\n",
        "            't_delta': self.t_delta\n",
        "        }\n",
        "        base_config = super(BatchRenormalization, self).get_config()\n",
        "\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "get_custom_objects().update({'BatchRenormalization': BatchRenormalization})\n",
        "\n",
        "#python train.py --model body_fusion,face_fusion,audio_feature,emotion_feature --epochs 300 --task emotion --fusion --fusion_type early\n",
        "#python train.py --model emotion_feature --epochs 100 --task emotion\n",
        "\n",
        "\n",
        "def load_custom_model(pretrained_model_path):\n",
        "    model = load_model(pretrained_model_path, \n",
        "                           custom_objects ={'ccc_metric':ccc_metric,\n",
        "                                            'ccc_loss': ccc_loss})\n",
        "    return model\n",
        "\n",
        "def train(istrain=True, model_type='audio_feature', saved_model_path=None, task='category',\n",
        "         batch_size=2, nb_epoch=200, learning_r=1e-3, show_plots=True, is_fusion=False,\n",
        "         fusion_type=None, pretrained=False):\n",
        "    \"\"\"\n",
        "    train the model\n",
        "    :param model: 'face_feature','audio_feature'\n",
        "    :param saved_model_path: saved_model path\n",
        "    :param task: 'aoursal','valence','category'\n",
        "    :param batch_size: 2\n",
        "    :param nb_epoch:2100\n",
        "    :return:s\n",
        "    \"\"\"\n",
        "    timestamp =  time.strftime('%Y-%m-%d-%H:%M:%S',time.localtime(time.time()))\n",
        "    # Helper: Save the model.\n",
        "    model_name = model_type\n",
        "    model_name = model_name.replace(':','-')\n",
        "    model_name = model_name.replace('[','')\n",
        "    model_name = model_name.replace(']','')\n",
        "    if ',' in model_name:\n",
        "        model_name = model_name.replace(',','__')\n",
        "        max_len = 200\n",
        "        if len(model_name) >= max_len:\n",
        "            model_name = model_name[:max_len]\n",
        "        model_name = 'fusion_' + fusion_type + '__' + model_name\n",
        "    if not os.path.exists(os.path.join('checkpoints', model_name)):\n",
        "        os.makedirs(os.path.join('checkpoints', model_name))\n",
        "    if task == 'category':\n",
        "        checkpointer = ModelCheckpoint(\n",
        "            monitor='val_acc',\n",
        "            #filepath = os.path.join('checkpoints', model, task+'-'+ str(timestamp)+'-'+'best.hdf5' ),\n",
        "            filepath = os.path.join('checkpoints', model_name, task + '-{val_acc:.3f}-{acc:.3f}.hdf5' ),\n",
        "            verbose=1,\n",
        "            save_best_only=True)\n",
        "        checkpointer_acc = ModelCheckpoint(\n",
        "            monitor='acc',\n",
        "            #filepath = os.path.join('checkpoints', model, task+'-'+ str(timestamp)+'-'+'best.hdf5' ),\n",
        "            filepath = os.path.join('checkpoints', model_name, task + '-{val_acc:.3f}-{acc:.3f}.hdf5' ),\n",
        "            verbose=1,\n",
        "            save_best_only=True)\n",
        "    else:\n",
        "        checkpointer_ccc = ModelCheckpoint(\n",
        "            monitor='val_ccc_metric',\n",
        "            #filepath = os.path.join('checkpoints', model, task+'-'+ str(timestamp)+'-'+'best.hdf5' ),\n",
        "            filepath = os.path.join('checkpoints', model_name, task + '-{val_ccc_metric:.3f}-{ccc_metric:.3f}.hdf5' ),\n",
        "            verbose=1,\n",
        "            save_best_only=True)\n",
        "        checkpointer_mse = ModelCheckpoint(\n",
        "            monitor='ccc_metric',\n",
        "            #filepath = os.path.join('checkpoints', model, task+'-'+ str(timestamp)+'-'+'best.hdf5' ),\n",
        "            filepath = os.path.join('checkpoints', model_name, task + '-{val_ccc_metric:.3f}-{ccc_metric:.3f}.hdf5' ),\n",
        "            verbose=1,\n",
        "            save_best_only=True)\n",
        "    \n",
        "    # Helper: TensorBoard\n",
        "    tb = TensorBoard(log_dir=os.path.join('logs', model_name))\n",
        "\n",
        "    # Helper: Stop when we stop learning.\n",
        "    early_stopper = EarlyStopping(patience=1000)\n",
        "    \n",
        "    # Helper: Save results.\n",
        "    \n",
        "    csv_logger = CSVLogger(os.path.join('logs', model_name , task +'-'+ \\\n",
        "        str(timestamp) + '.log'))\n",
        "\n",
        "    # Get the data and process it.\n",
        "    # seq_length for the sentence\n",
        "    seq_length = 20\n",
        "    dataset = DataSet(\n",
        "        istrain = istrain,\n",
        "        model = model_type,\n",
        "        task = task,\n",
        "        seq_length=seq_length,\n",
        "        model_name=model_name,\n",
        "        is_fusion=is_fusion\n",
        "        )\n",
        "\n",
        "    # Get the model.\n",
        "    model = None\n",
        "    if pretrained:\n",
        "        model_weights_path = get_best_model(model_name)\n",
        "        if model_weights_path:\n",
        "            print('USING MODEL', model_weights_path)\n",
        "            model = load_model(model_weights_path)\n",
        "        # model_file = os.path.join('models',model_name + '.hdf5')\n",
        "        # if os.path.exists(model_file):\n",
        "        #     model = load_model(model_file)\n",
        "        # else:\n",
        "        #     print('No trained model found')\n",
        "    if model is None:\n",
        "        rm = ResearchModels(\n",
        "                istrain = istrain,\n",
        "                model = model_type, \n",
        "                seq_length = seq_length, \n",
        "                saved_path=saved_model_path, \n",
        "                task_type= task,\n",
        "                learning_r = learning_r,\n",
        "                model_name=model_name,\n",
        "                is_fusion=is_fusion,\n",
        "                fusion_type=fusion_type\n",
        "                )\n",
        "        model = rm.model\n",
        "    # Get training and validation data.\n",
        "    x_train, y_train, train_name_list = dataset.get_all_sequences_in_memory('Train')\n",
        "    x_valid, y_valid, valid_name_list= dataset.get_all_sequences_in_memory('Validation')\n",
        "    x_test, y_test, test_name_list = dataset.get_all_sequences_in_memory('Test')\n",
        "    if task == 'category':\n",
        "        y_train = to_categorical(y_train)\n",
        "        y_valid = to_categorical(y_valid)\n",
        "        y_test = to_categorical(y_test)\n",
        "\n",
        "    # Fit!\n",
        "    # Use standard fit\n",
        "    print('Size', len(x_train), len(y_train), len(x_valid), len(y_valid), len(x_test), len(y_test))\n",
        "\n",
        "    if task == 'category':\n",
        "        callbacks = [tb, csv_logger,  checkpointer, checkpointer_acc]\n",
        "    else:\n",
        "        callbacks = [tb, csv_logger,  checkpointer_ccc, checkpointer_mse]\n",
        "\n",
        "    history = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_valid,y_valid),\n",
        "        verbose=0,\n",
        "        callbacks=callbacks,\n",
        "        #callbacks=[tb, early_stopper, csv_logger,  checkpointer],\n",
        "        #callbacks=[tb, lrate, csv_logger,  checkpointer],\n",
        "        epochs=nb_epoch)\n",
        "    \n",
        "    # find the current best model and get its prediction on validation set\n",
        "    model_weights_path = get_best_model(model_name, task)\n",
        "    #model_weights_path = os.path.join('checkpoints', model_name, task + '-' + str(nb_epoch) + '-' + str(timestamp) + '-' + 'best.hdf5' )\n",
        "    print('model_weights_path', model_weights_path)\n",
        "\n",
        "    if model_weights_path:\n",
        "        best_model = load_custom_model(model_weights_path)\n",
        "    else:\n",
        "        best_model = model\n",
        "    \n",
        "\n",
        "    y_valid_pred = best_model.predict(x_valid)\n",
        "    y_valid_pred = np.squeeze(y_valid_pred)\n",
        "    \n",
        "    y_train_pred = best_model.predict(x_train)\n",
        "    y_train_pred = np.squeeze(y_train_pred)\n",
        "\n",
        "    y_test_pred = best_model.predict(x_test)\n",
        "    y_test_pred = np.squeeze(y_test_pred)\n",
        "\n",
        "    #calculate the ccc and mse\n",
        "\n",
        "    if not os.path.exists('results'):\n",
        "        os.mkdir('results')\n",
        "    filename = os.path.join('results', model_name+'__'+str(nb_epoch)+'_'+task+'.txt')\n",
        "    log_path = os.path.join('logs', model_name , task +'-'+ str(timestamp) + '.log')\n",
        "    record = {'model_name':model_name, 'fusion_type':fusion_type, 'task': task}\n",
        "    if task in ['arousal', 'valence']:\n",
        "        ccc_val = ccc(y_valid, y_valid_pred)[0]\n",
        "        mse_val = mse_metric(y_valid, y_valid_pred)\n",
        "        ccc_train = ccc(y_train, y_train_pred)[0]\n",
        "        mse_train = mse_metric(y_train, y_train_pred)\n",
        "        ccc_test = ccc(y_test, y_test_pred)[0]\n",
        "        mse_test = mse_metric(y_test, y_test_pred)\n",
        "        print(\"The CCC in validation set is {}\".format(ccc_val))\n",
        "        print(\"The mse in validation set is {}\".format(mse_val))\n",
        "        print(\"The CCC in train set is {}\".format(ccc_train))\n",
        "        print(\"The mse in train set is {}\".format(mse_train))\n",
        "        print(\"The CCC in test set is {}\".format(ccc_test))\n",
        "        print(\"The mse in test set is {}\".format(mse_test))\n",
        "        all_results.append({'field':'ccc_val', 'value': ccc_val, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'mse_val', 'value': mse_val, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'ccc_train', 'value': ccc_train, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'mse_train', 'value': mse_train, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'ccc_test', 'value': ccc_test, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'mse_test', 'value': mse_test, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(str([ccc_val, mse_val, ccc_train, mse_train]))\n",
        "        display_true_vs_pred([y_valid,y_train], [y_valid_pred, y_train_pred],log_path, task, model_name, show_plots, timestamp, nb_epoch)\n",
        "    if task == 'category':\n",
        "        f1_score = f1(y_valid, y_valid_pred)\n",
        "        f1_score_test = f1(y_test, y_test_pred)\n",
        "        acc_val = model.evaluate(x_valid, y_valid, verbose=1)[1]\n",
        "        acc_train = model.evaluate(x_train, y_train, verbose=1)[1]\n",
        "        acc_test = model.evaluate(x_test, y_test, verbose=1)[1]\n",
        "        print(\"F1 score in validation set is {}\".format(f1_score))\n",
        "        print(\"F1 score in test set is {}\".format(f1_score_test))\n",
        "        print(\"Val acc is {}\".format(acc_val))\n",
        "        print(\"Train acc is {}\".format(acc_train))\n",
        "        print(\"Test acc is {}\".format(acc_test))\n",
        "        all_results.append({'field':'f1_score', 'value': f1_score, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'f1_score_test', 'value': f1_score_test, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'acc_val', 'value': acc_val, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'acc_train', 'value': acc_train, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        all_results.append({'field':'acc_test', 'value': acc_test, 'model_name':model_name, 'fusion_type':fusion_type, 'task': task})\n",
        "        plot_acc(history, model_name, timestamp, show_plots, nb_epoch)\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(str([acc_val, acc_train, acc_test, f1_score, f1_score_test]))\n",
        "        # display the prediction and true label\n",
        "        display_true_vs_pred_emotion([y_valid, y_train, y_test], [y_valid_pred, y_train_pred, y_test_pred],log_path, task, model_name, [acc_val, acc_train, acc_test], show_plots, timestamp, nb_epoch)\n",
        "    \n",
        "\n",
        "def get_best_model(model_name, task):\n",
        "    best_model = None\n",
        "    model_path = os.path.join('checkpoints', model_name)\n",
        "    if os.path.exists(model_path):\n",
        "        files = glob.glob(os.path.join(model_path, task + '*'))\n",
        "        if len(files) > 0:\n",
        "            files.sort()\n",
        "            best_model = files[-1]\n",
        "    return best_model\n",
        "    \n",
        "def train_model(args):\n",
        "    is_train = True\n",
        "    # pdb.set_trace()\n",
        "    show_plots = True\n",
        "    pretrained = args['pretrained']\n",
        "    if ',' in args['task']:\n",
        "        tasks = args['task'].split(',')\n",
        "    else:\n",
        "        if args['task'] == 'all':\n",
        "            show_plots = False\n",
        "            tasks = ['category', 'arousal', 'valence']\n",
        "        else:\n",
        "            tasks = [args['task']]\n",
        "    if args['fusion']:\n",
        "        if args['fusion_type'] == 'all':\n",
        "            show_plots = False\n",
        "            fusion_types = ['early', 'late']\n",
        "        else:\n",
        "            fusion_types = [args['fusion_type']]\n",
        "        if ';' in args['model']:\n",
        "            show_plots = False\n",
        "            models = args['model'].split(';')\n",
        "        else:\n",
        "            models = [args['model']]\n",
        "        for model in models:\n",
        "            submodels = model.split(',')\n",
        "            if len(submodels) > 1:\n",
        "                for fusion_type in fusion_types:\n",
        "                    for task in tasks:\n",
        "                        train(istrain=is_train, model_type = model, saved_model_path=args['pretrained_model_path'], task = task,\n",
        "                        batch_size=args['batch_size'], nb_epoch=args['epochs'], learning_r = args['learning_rate'], show_plots=show_plots,\n",
        "                        is_fusion=True, fusion_type=fusion_type, pretrained=pretrained)\n",
        "                        gc.collect()\n",
        "            else:\n",
        "                print('Fusion models must have at least two models separated by comma')\n",
        "    else:\n",
        "        if ',' in args['model']:\n",
        "            models = args['model'].split(',')\n",
        "        else:\n",
        "            if args['model'] == 'all':\n",
        "                models = ['audio_feature', 'body_feature', 'body_visual', 'body_fusion', 'emotion_feature', 'face_feature', 'face_visual', 'face_fusion', 'word_feature', 'word_mpqa', 'word_fusion']\n",
        "            else:\n",
        "                models = [args['model']]\n",
        "        if len(tasks) > 1:\n",
        "            show_plots = False\n",
        "        if len(models) > 1:\n",
        "            show_plots = False\n",
        "        for model in models:\n",
        "            for task in tasks:\n",
        "                print(model, task)\n",
        "                train(istrain=is_train, model_type = model, saved_model_path=args['pretrained_model_path'], task = task,\n",
        "                    batch_size=args['batch_size'], nb_epoch=args['epochs'], learning_r = args['learning_rate'], show_plots=show_plots,\n",
        "                    is_fusion=False, pretrained=pretrained)\n",
        "\n",
        "def print_results():\n",
        "  display(HTML(tabulate.tabulate(all_results, tablefmt='html')))\n",
        "                \n",
        "def get_args():\n",
        "  return {\n",
        "    'model': 'audio_feature',\n",
        "    'task': 'all',\n",
        "    'fusion': False,\n",
        "    'fusion_type': 'all',\n",
        "    'epochs': 100,\n",
        "    'pretrained': False,\n",
        "    'pretrained_model_path': None,\n",
        "    'is_train': True,\n",
        "    'learning_rate': 1e-3,\n",
        "    'batch_size': 300\n",
        "  }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twVTL0qv5BAs",
        "colab_type": "text"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpgkXwNrYs26",
        "colab_type": "code",
        "outputId": "4b2131c7-c7d5-4059-e396-a80068dd0227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8761
        }
      },
      "source": [
        "%%time\n",
        "data_folder = 'features'\n",
        "\n",
        "args = get_args()\n",
        "#args['task'] = 'all'\n",
        "args['task'] = 'valence'\n",
        "args['epochs'] = 100\n",
        "#args['fusion'] = True\n",
        "#args['fusion_type'] = 'early'\n",
        "args['model'] = 'audio_feature' \n",
        "#args['model'] = 'audio_feature,word_fusion,face_fusion;audio_feature,word_fusion,face_fusion,body_fusion;audio_feature,word_fusion,face_fusion,body_fusion,emotion_feature'\n",
        "\n",
        "if will_train_model:\n",
        "  all_results = []\n",
        "  train_model(args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('audio_feature', 'valence')\n",
            "The dataset has been split to: train set:231 videos, 2442 utterances; validation set: 60 videos, 617 utterances. \n",
            "Evaluation for test set: 204 videos, 2229 utterances. \n",
            "WARNING:tensorflow:From /home/tox/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /home/tox/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "audio_feature_BN_1_94 (Batch (None, 1582)              6328      \n",
            "_________________________________________________________________\n",
            "audio_feature_hidden_94_hidd (None, 256)               405248    \n",
            "_________________________________________________________________\n",
            "audio_feature_hidden_94_acti (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "audio_feature_hidden_94_BN ( (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "audio_feature_hidden_94_drop (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "audio_feature_94_decision_la (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 412,857\n",
            "Trainable params: 409,181\n",
            "Non-trainable params: 3,676\n",
            "_________________________________________________________________\n",
            "None\n",
            "Loading Train dataset with 231 videos.\n",
            "Loading Validation dataset with 60 videos.\n",
            "Loading Test dataset with 204 videos.\n",
            "Error when access to d1dae148e utterance_1 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_2 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_3 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_4 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_5 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_6 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_7 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_8 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_9 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_10 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_11 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_12 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_13 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_14 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_18 in audio_feature\n",
            "Error when access to 5ba7890ab_1 utterance_19 in audio_feature\n",
            "('Size', 2442, 2442, 617, 617, 2213, 2213)\n",
            "WARNING:tensorflow:From /home/tox/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "Epoch 00001: val_ccc_metric improved from inf to 0.20112, saving model to checkpoints/audio_feature/valence-0.201-0.096.hdf5\n",
            "\n",
            "Epoch 00001: ccc_metric improved from inf to 0.09587, saving model to checkpoints/audio_feature/valence-0.201-0.096.hdf5\n",
            "\n",
            "Epoch 00002: val_ccc_metric improved from 0.20112 to 0.18122, saving model to checkpoints/audio_feature/valence-0.181-0.214.hdf5\n",
            "\n",
            "Epoch 00002: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00003: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00003: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00004: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00004: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00005: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00005: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00006: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00006: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00007: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00007: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00008: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00008: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00009: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00009: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00010: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00010: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00011: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00011: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00012: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00012: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00013: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00013: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00014: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00014: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00015: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00015: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00016: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00016: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00017: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00017: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00018: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00018: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00019: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00019: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00020: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00020: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00021: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00021: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00022: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00022: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00023: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00023: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00024: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00024: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00025: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00025: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00026: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00026: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00027: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00027: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00028: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00028: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00029: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00029: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00030: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00030: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00031: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00031: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00032: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00032: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00033: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00033: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00034: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00034: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00035: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00035: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00036: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00036: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00037: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00037: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00038: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00038: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00039: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00039: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00040: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00040: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00041: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00041: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00042: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00042: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00043: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00043: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00044: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00044: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00045: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00045: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00046: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00046: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00047: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00047: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00048: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00048: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00049: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00049: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00050: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00050: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00051: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00051: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00052: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00052: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00053: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00053: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00054: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00054: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00055: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00055: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00056: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00056: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00057: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00057: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00058: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00058: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00059: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00059: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00060: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00060: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00061: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00061: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00062: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00062: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00063: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00063: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00064: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00064: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00065: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00065: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00066: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00066: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00067: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00067: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00068: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00068: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00069: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00069: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00070: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00070: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00071: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00071: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00072: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00072: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00073: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00073: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00074: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00074: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00075: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00075: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00076: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00076: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00077: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00077: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00078: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00078: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00079: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00079: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00080: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00080: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00081: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00081: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00082: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00082: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00083: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00083: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00084: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00084: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00085: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00085: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00086: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00086: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00087: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00087: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00088: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00088: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00089: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00089: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00090: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00090: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00091: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00091: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00092: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00092: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00093: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00093: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00094: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00094: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00095: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00095: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00096: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00096: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00097: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00097: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00098: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00098: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00099: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00099: ccc_metric did not improve from 0.09587\n",
            "\n",
            "Epoch 00100: val_ccc_metric did not improve from 0.18122\n",
            "\n",
            "Epoch 00100: ccc_metric did not improve from 0.09587\n",
            "('model_weights_path', 'checkpoints/audio_feature/valence-0.201-0.096.hdf5')\n",
            "The CCC in validation set is 0.215680815024\n",
            "The mse in validation set is 0.48592187578\n",
            "The CCC in train set is 0.249011830223\n",
            "The mse in train set is 0.481890425592\n",
            "The CCC in test set is 0.199247075741\n",
            "The mse in test set is 0.47692677341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.8 s, sys: 3.67 s, total: 24.5 s\n",
            "Wall time: 14 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGFGvZ3Ww1MS",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivH-NeXnwlf3",
        "colab_type": "code",
        "outputId": "fc4104f5-9454-4616-e373-ffff22ea6dda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "print_results()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<tbody>\n",
              "<tr><td></td><td>ccc_val  </td><td>valence</td><td>audio_feature</td><td style=\"text-align: right;\">0.215681</td></tr>\n",
              "<tr><td></td><td>mse_val  </td><td>valence</td><td>audio_feature</td><td style=\"text-align: right;\">0.485922</td></tr>\n",
              "<tr><td></td><td>ccc_train</td><td>valence</td><td>audio_feature</td><td style=\"text-align: right;\">0.249012</td></tr>\n",
              "<tr><td></td><td>mse_train</td><td>valence</td><td>audio_feature</td><td style=\"text-align: right;\">0.48189 </td></tr>\n",
              "<tr><td></td><td>ccc_test </td><td>valence</td><td>audio_feature</td><td style=\"text-align: right;\">0.199247</td></tr>\n",
              "<tr><td></td><td>mse_test </td><td>valence</td><td>audio_feature</td><td style=\"text-align: right;\">0.476927</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEIGQBZHRADe",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4VmDLhZvtl",
        "colab_type": "text"
      },
      "source": [
        "In this paper, we present MultiAffect a reproducible research framework for mul- timodal affect and action recognition. To achieve this goal we designed a multi- purpose interactive notebook able to properly configure an environment that can extract features, train, and test a model from annotated long-term videos. The framework allows users to perform these tasks from different modalities such as text, audio, instant emotions, face images, and body skeleton. Unimodal and multimodal approaches with different fusion strategy can be easily configured and processed. Classification and regression tasks over videos can be trained and evaluated. We tested the framework in video action and affect recognition tasks. The tool was able to perform both actions by only adjusting the configura- tion settings. The results of the vanilla algorithms implemented resulted average in the regression task and outstanding in the classification task. The framework is available online, can be executed with no requirements in the client side and free of charge. Our aim is to boost reproducible research in this area by applying the design principles presented in this work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnoTQdvGRFOR",
        "colab_type": "text"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tUNYOYsRHcc",
        "colab_type": "text"
      },
      "source": [
        "[1] Multiaffect reproducible research framework - colaboratory. https://bit.ly/ multiaffect. (Accessed on 05/30/2019)\n",
        "\n",
        "[2] oarriaga/face classification: Real-time face detection and emotion/gender classifi- cation using fer2013/imdb datasets with a keras cnn model and opencv. https://github.com/oarriaga/face_classification. (Accessed on 04/28/2019)\n",
        "\n",
        "[3] Omg-emotion challenge. https://www2.informatik.uni-hamburg.de/wtm/ OMG-EmotionChallenge/. (Accessed on 05/29/2019)\n",
        "\n",
        "[4] Openface. https://cmusatyalab.github.io/openface/. (Accessed  on 04/26/2019)\n",
        "\n",
        "[5] opensmile/emobase2010.conf at master naxingyu/opensmile. https://github. com/naxingyu/opensmile/blob/master/config/emobase2010.conf. (Accessed on 04/26/2019)\n",
        "\n",
        "[6] petercunha/emotion: Recognizes human faces and their corresponding emotions from a video or webcam feed. powered by opencv and deep learning. https://github.com/petercunha/Emotion. (Accessed on 04/28/2019)\n",
        "\n",
        "[7] priya-dwivedi/face and emotion detection.\thttps://github.com/ priya-dwivedi/face_and_emotion_detection. (Accessed on 04/28/2019)\n",
        "\n",
        "[8] thoughtworksarts/emopy: A deep neural net toolkit for emotion analysis via fa- cial expression recognition (fer). https://github.com/thoughtworksarts/EmoPy. (Accessed on 04/28/2019)\n",
        "\n",
        "[9] Baker, M.: 1,500 scientists lift the lid on reproducibility. Nature News 533(7604), 452 (2016)\n",
        "\n",
        "[10] Baltruˇsaitis, T., Robinson, P., Morency, L.P.: 3d constrained local model for rigid and non-rigid facial tracking. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2610–2617. IEEE (2012)\n",
        "\n",
        "[11] Baltruˇsaitis, T., Robinson, P., Morency, L.P.: Openface: an open source facial be- havior analysis toolkit. In: 2016 IEEE Winter Conference on Applications of Com- puter Vision (WACV), pp. 1–10. IEEE (2016)\n",
        "\n",
        "[12] Barros, P.,  Churamani, N., Lakomkin, E., Siqueira, H., Sutherland, A., Wermter,  S.: The omg-emotion behavior dataset. In: 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1–7. IEEE (2018)\n",
        "\n",
        "[13] Bengio, Y., Delalleau, O., Le Roux, N.: The curse of dimensionality for local kernel machines. Techn. Rep 1258 (2005)\n",
        "\n",
        "[14] Bengio, Y., LeCun, Y., et al.: Scaling learning algorithms towards ai. Large-scale kernel machines 34(5), 1–41 (2007)\n",
        "\n",
        "[15] Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: Openpose: realtime multi-person 2d pose estimation using part affinity fields. arXiv preprint arXiv:1812.08008 (2018)\n",
        "\n",
        "[16] Delbrouck, J.B.: Transformer for emotion recognition. arXiv preprint arXiv:1805.02489 (2018)\n",
        "\n",
        "[17] Deng, D., Zhou, Y., Pi, J., Shi, B.E.: Multimodal utterance-level affect analysis using visual, audio and text features. arXiv preprint arXiv:1805.00625 (2018)\n",
        "\n",
        "[18] Ding, X., Liu, B., Yu, P.S.: A holistic lexicon-based approach to opinion mining. In: Proceedings of the 2008 international conference on web search and data mining, pp. 231–240. ACM (2008)\n",
        "\n",
        "[19] D’mello, S.K., Kory, J.: A review and meta-analysis of multimodal affect detection systems. ACM Computing Surveys (CSUR) 47(3), 43 (2015)\n",
        "\n",
        "[20] Ekman, P.: Wallance. v. friesen.” facial action coding system. ConSultingPsychol- ogists PreSSInc (1978)\n",
        "\n",
        "[21] Ekman, P., Friesen, W.V.: The repertoire of nonverbal behavior: Categories, ori- gins, usage, and coding. semiotica 1(1), 49–98 (1969)\n",
        "\n",
        "[22] Eyben, F., Wollmer, M., Schuller, B.: Opensmile: the munich versatile and fast open-source audio feature extractor. In: Proceedings of the 18th ACM international conference on Multimedia, pp. 1459–1462. ACM (2010)\n",
        "\n",
        "[23] Fayek, H.M., Lech, M., Cavedon, L.: Towards real-time speech emotion recogni- tion using deep neural networks. In: 2015 9th international conference on signal processing and communication systems (ICSPCS), pp. 1–5. IEEE (2015)\n",
        "\n",
        "[24] Ferreira, P.M., Pernes, D., Fernandes, K., Rebelo, A., Cardoso, J.S.: Dimensional emotion recognition using visual and textual cues. arXiv preprint arXiv:1805.01416 (2018)\n",
        "\n",
        "[25] Huang, Z., Dong, M., Mao, Q., Zhan, Y.: Speech emotion recognition using cnn. In: Proceedings of the 22nd ACM international conference on Multimedia, pp. 801–804. ACM (2014)\n",
        "\n",
        "[26] Kluyver, T., Ragan-Kelley, B., P´erez, F., Granger, B.E., Bussonnier, M., Frederic, J., Kelley, K., Hamrick, J.B., Grout, J., Corlay, S., et al.: Jupyter notebooks-a publishing format for reproducible computational workflows. In: ELPUB, pp. 87– 90 (2016)\n",
        "\n",
        "[27] Lawrence, I., Lin, K.: Assay validation using the concordance correlation coeffi- cient. Biometrics pp. 599–604 (1992)\n",
        "\n",
        "[28] Munaf`o, M.R., Nosek, B.A., Bishop, D.V., Button, K.S., Chambers, C.D., Du Sert, N.P., Simonsohn, U., Wagenmakers, E.J., Ware, J.J., Ioannidis, J.P.: A manifesto  for reproducible science. Nature human behaviour 1(1), 0021 (2017)\n",
        "\n",
        "[29] Ortony, A., Turner, T.J.: What’s basic about basic emotions? Psychological review 97(3), 315 (1990)\n",
        "\n",
        "[30] Pantic, M., Sebe, N., Cohn, J.F., Huang, T.: Affective multimodal human-computer interaction. In: Proceedings of the 13th annual ACM international conference on Multimedia, pp. 669–676. ACM (2005)\n",
        "\n",
        "[31] Parkhi, O.M., Vedaldi, A., Zisserman, A., et al.: Deep face recognition. In: bmvc, vol. 1, p. 6 (2015)\n",
        "\n",
        "[32] Peng, S., Zhang, L., Ban, Y., Fang, M., Winkler, S.: A deep network for arousal-valence emotion prediction with acoustic-visual cues. arXiv preprint arXiv:1805.00638 (2018)\n",
        "\n",
        "[33] Picard, R.W.: A ective computing (1997)\n",
        "\n",
        "[34] Ranganathan, H., Chakraborty, S., Panchanathan, S.: Multimodal emotion recog- nition using deep learning architectures. In: 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1–9. IEEE (2016)\n",
        "\n",
        "[35] Rouast, P.V., Adam, M., Chiong, R.: Deep learning for human affect recognition: Insights and new developments. IEEE Transactions on Affective Computing (2019)\n",
        "\n",
        "[36] Russell, J.A.: A circumplex model of affect. Journal of personality and social psychology 39(6), 1161 (1980)\n",
        "\n",
        "[37] Schuller,  B.,  Steidl,  S.,  Batliner,  A.,  Burkhardt,  F.,  Devillers,  L.,  Mu¨ller,  C., Narayanan, S.S.: The interspeech 2010 paralinguistic challenge. In: Eleventh An- nual Conference of the International Speech Communication Association (2010)\n",
        "\n",
        "[38] Soleymani, M., Pantic, M., Pun, T.: Multimodal emotion recognition in response  to videos. IEEE transactions on affective computing 3(2), 211–223 (2012)\n",
        "\n",
        "[39] Triantafyllopoulos, A., Sagha, H., Eyben, F., Schuller, B.: audeering’s approach to the one-minute-gradual emotion challenge. arXiv preprint arXiv:1805.01222 (2018)\n",
        "\n",
        "[40] Wilson, T., Wiebe, J., Hoffmann, P.: Recognizing contextual polarity in phrase- level sentiment analysis. In: Proceedings of Human Language Technology Con- ference and Conference on Empirical Methods in Natural Language Processing (2005)\n",
        "\n",
        "[41] W¨ollmer,  M.,  Kaiser,  M.,  Eyben,  F.,  Schuller,  B.,  Rigoll,  G.:  Lstm-modeling  of continuous emotions in an audiovisual affect recognition framework. Image and Vision Computing 31(2), 153–163 (2013)\n",
        "\n",
        "[42] Zhang, S., Zhang, S., Huang, T., Gao, W., Tian, Q.: Learning affective features with a hybrid deep model for audio–visual emotion recognition. IEEE Transactions on Circuits and Systems for Video Technology 28(10), 3030–3043 (2018)\n",
        "\n",
        "[43] Zheng, Z., Cao, C., Chen, X., Xu, G.: Multimodal emotion recognition for one- minute-gradual emotion challenge. arXiv preprint arXiv:1805.01060 (2018)\n",
        "\n",
        "[44] Video blooper dataset for automatic video editing — kaggle.https://www.kaggle.com/toxtli/video-blooper-dataset-for-automatic-video-editing.(Ac-cessed on 05/31/2019)\n"
      ]
    }
  ]
}